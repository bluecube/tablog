{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-softball",
   "metadata": {},
   "source": [
    "# String predictor\n",
    "\n",
    "Tablog encodes column name strings in headers and we want to compress them at least a bit -- the strings should not be the dominant part of the dataset, but still saving few bytes is always good.\n",
    "\n",
    "This file contains experiments that should help determining an appropriate compression algorithm.\n",
    "\n",
    "Goals for this part are (in order of importance):\n",
    "1. fast and light\n",
    "2. simple-ishto implement\n",
    "3. compresses well enough to make me happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86d5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "import heapq\n",
    "import itertools\n",
    "import string\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e07dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb24b0",
   "metadata": {},
   "source": [
    "## Manual test data\n",
    "These are just a couple of strings that I feel will be representative of the intended use of Tablog.\n",
    "Used mostly to get an intuitive feel of how the compression behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88310baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test_data = [\n",
    "    \"deadbeef\", \"hello_world\", \"voltage\", \"current\", \"left\", \"column58\", \"left_motor_speed\", \"kP\", \"right_motor_kD\",\n",
    "    \"POWER!\", \"timestamp\", \"AverageTicks\", \"temperature\", \"MCU-power\", \"alpha\", \"beta\", \"omega\", \"velocityX\",\n",
    "    \"!@#$%^&*()\", \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-parking",
   "metadata": {},
   "source": [
    "## Harvesting some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc6b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_identifiers(path):\n",
    "    \"\"\"Recursively go through all source-looking files in the directory and count\n",
    "    all strings that look like identifiers.\"\"\"\n",
    "    \n",
    "    extensions = (\".cpp\", \".hpp\", \".c\", \".h\", \".py\", \".sh\")\n",
    "    identifier_regex = re.compile(r\"(?<![a-zA-Z0-9_])[a-zA-Z_][a-zA-Z0-9_]*\")\n",
    "    \n",
    "    ret = collections.Counter()\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if not file.endswith(extensions):\n",
    "                continue\n",
    "            path = os.path.join(root, file)\n",
    "            with open(path, \"r\") as fp:\n",
    "                for line in fp:\n",
    "                    for match in identifier_regex.finditer(line):\n",
    "                        ret[match.group(0)] += 1\n",
    "                        \n",
    "    return ret\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-academy",
   "metadata": {},
   "source": [
    "Load strings that look like identifiers from Tablog sources and (potentionally) also from Linux kernel sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0c246a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for identifers in /home/cube/development/c/tablog/encoder\n",
      "   Found 894 unique identifers\n",
      "Looking for identifers in /home/cube/development/c/tablog/decoder\n",
      "   Found 573 unique identifers\n",
      "Looking for identifers in /home/cube/development/c/tablog/integration_tests\n",
      "   Found 241 unique identifers\n"
     ]
    }
   ],
   "source": [
    "data = collections.Counter()\n",
    "for directory in [\n",
    "    \"/home/cube/development/c/tablog/encoder\",\n",
    "    \"/home/cube/development/c/tablog/decoder\",\n",
    "    \"/home/cube/development/c/tablog/integration_tests\",\n",
    "    #\"/usr/src/linux\"\n",
    "]:\n",
    "    print(\"Looking for identifers in\", directory)\n",
    "    dir_data = load_identifiers(directory)\n",
    "    print(\"   Found\", len(dir_data), \"unique identifers\")\n",
    "    \n",
    "    data += dir_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d8ffc",
   "metadata": {},
   "source": [
    "Because of the primitive way the test data was included, it's significantly biased by the programming language, so we manually remove some of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88ad22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('std', 230), ('self', 203), ('T', 138), ('if', 127), ('the', 123), ('i', 123), ('data', 118), ('return', 111), ('def', 99), ('b', 86), ('include', 83), ('const', 82), ('a', 81), ('value', 79), ('make_pair', 79), ('auto', 74), ('for', 73), ('output', 73), ('else', 65), ('hypothesis', 62), ('REQUIRE', 61), ('assert', 60), ('int_type', 60), ('is', 57), ('c', 56), ('typename', 54), ('bitWriter', 50), ('template', 49), ('of', 48), ('and', 48), ('import', 48), ('in', 46), ('n', 45), ('strategies', 45), ('void', 43), ('expected', 43), ('string_view', 42), ('type', 42), ('max', 41), ('to', 40), ('write', 39), ('from', 39), ('framing', 38), ('that', 38), ('unsigned', 38), ('SECTION', 37), ('v', 37), ('ret', 36), ('tablog', 36), ('byte', 36), ('values', 36), ('bits', 35), ('bw', 34), ('constexpr', 33), ('uint32_t', 33), ('br', 32), ('hpp', 31), ('BW', 31), ('bytes', 31), ('buffer', 31), ('class', 30), ('first', 30), ('append', 30), ('uint8_t', 29), ('s', 29), ('pytest', 29), ('detail', 28), ('f', 28), ('min', 28), ('decoder', 28), ('end', 27), ('string', 27), ('signed', 27), ('uint16_t', 26), ('args', 26)]\n"
     ]
    }
   ],
   "source": [
    "print(data.most_common(75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b4398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\n",
    "    b\"std\", b\"if\", b\"include\", b\"self\", b\"return\", b\"def\", b\"auto\", b\"define\", b\"struct\",\n",
    "    b\"static\", b\"break\", b\"goto\", b\"int\", b\"void\", b\"unsigned\", b\"const\",\n",
    "]:\n",
    "    del data[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f17241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('std', 230), ('self', 203), ('T', 138), ('if', 127), ('the', 123), ('i', 123), ('data', 118), ('return', 111), ('def', 99), ('b', 86), ('include', 83), ('const', 82), ('a', 81), ('value', 79), ('make_pair', 79), ('auto', 74), ('for', 73), ('output', 73), ('else', 65), ('hypothesis', 62), ('REQUIRE', 61), ('assert', 60), ('int_type', 60), ('is', 57), ('c', 56), ('typename', 54), ('bitWriter', 50), ('template', 49), ('of', 48), ('and', 48), ('import', 48), ('in', 46), ('n', 45), ('strategies', 45), ('void', 43), ('expected', 43), ('string_view', 42), ('type', 42), ('max', 41), ('to', 40), ('write', 39), ('from', 39), ('framing', 38), ('that', 38), ('unsigned', 38), ('SECTION', 37), ('v', 37), ('ret', 36), ('tablog', 36), ('byte', 36), ('values', 36), ('bits', 35), ('bw', 34), ('constexpr', 33), ('uint32_t', 33), ('br', 32), ('hpp', 31), ('BW', 31), ('bytes', 31), ('buffer', 31), ('class', 30), ('first', 30), ('append', 30), ('uint8_t', 29), ('s', 29), ('pytest', 29), ('detail', 28), ('f', 28), ('min', 28), ('decoder', 28), ('end', 27), ('string', 27), ('signed', 27), ('uint16_t', 26), ('args', 26)]\n"
     ]
    }
   ],
   "source": [
    "print(data.most_common(75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65b53e",
   "metadata": {},
   "source": [
    "## Harvesting test data v2\n",
    "The general idea here is to start with actual english words, all lower case and then manually extend this set with simple operations with eyeballed weights. These operations will include:\n",
    "1. Capitalization\n",
    "  1. All caps\n",
    "  1. First cap\n",
    "1. Most common last-first letter combinations, separated with\n",
    "  1. spaces\n",
    "  1. camelCase\n",
    "  1. snake_case\n",
    "  1. SCREAMING_SNAKE_CASE\n",
    "  1. dashes\n",
    "1. Single digits (?)\n",
    "  - Increase probability of small digits (Benford's law) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963e275c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddfade7c6fd4494993cd94d899ccfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667a280c706491c81a35df7dd75d823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a:   0%|          | 0/343487895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6232a2759e844fe88f705b50e19df66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b:   0%|          | 0/247964820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ac71d7c0a941d08078d6683e6a6d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c:   0%|          | 0/393712861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157d4293c68d46059a8f86dc8b385ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "d:   0%|          | 0/239389478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb54c1872e4855b9b0ee080f4713a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "e:   0%|          | 0/204996476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9b9231bd2847cf8dd892ae01851af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "f:   0%|          | 0/185305580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58983098388445a49a109a09f751716b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "g:   0%|          | 0/160767731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0c4093219c4262b8a56227080ee7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "h:   0%|          | 0/183438682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60931e8848104466a6857425261d9992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "i:   0%|          | 0/203553215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c427350bf744ee8a463035e7bf83ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/64839808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c97a6e056f746088766774cc52aa75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "k:   0%|          | 0/109307817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962e3783bd494354805ecf914b83e685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "l:   0%|          | 0/188008415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c99331ffee749d2b069153557afdd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "m:   0%|          | 0/289750659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf563820c0f48be88b210f5afb5ef4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n:   0%|          | 0/141931087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a5f4de115c4fde908ff646f61d466a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "o:   0%|          | 0/139934943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0661b96ae99945d6af1f0c71b39894b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "p:   0%|          | 0/357808228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b8dd5a613f42f696cbc6b88f93a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "q:   0%|          | 0/26608642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0739e1c0690e462ca1425e6e60d2dcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r:   0%|          | 0/216873218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145d44dde56d4e30a86434052518fe75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "s:   0%|          | 0/444315601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04eba808d4b407f8ed239867444ac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "t:   0%|          | 0/265071292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008e59fdb67b4494b10016d434a422c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "u:   0%|          | 0/88124330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f1524110324bf481e35fd2c5777f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v:   0%|          | 0/108848534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cbe26263fe4764ac2f10c1aec6c663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "w:   0%|          | 0/118835349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81361f84a1943e8b63e95f6157ff20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "x:   0%|          | 0/14649720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f424fd164454d8780071b664004846d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "y:   0%|          | 0/26595711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01cda1a5c124b6eaf4d775823889eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "z:   0%|          | 0/27324007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def google_books_1grams_words():\n",
    "    ret = collections.Counter()\n",
    "    good_string_re = re.compile(r\"^([a-zA-Z]*)\\t[^\\t]*\\t([0-9]*)\")\n",
    "    for letter in tqdm(string.ascii_lowercase):\n",
    "        r = requests.get(f\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-{letter}.gz\", stream=True)\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=file_size, desc=letter) as compressed_response_file:\n",
    "            with gzip.open(compressed_response_file, \"rt\") as response_file:\n",
    "                for line in response_file:\n",
    "                    if match := good_string_re.match(line):\n",
    "                        ret[match[1].lower()] += int(match[2])\n",
    "                        \n",
    "    return ret\n",
    "\n",
    "english_words = google_books_1grams_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5760f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 26548583149),\n",
       " ('of', 15482969531),\n",
       " ('and', 11315969857),\n",
       " ('to', 9673642739),\n",
       " ('in', 8445476198),\n",
       " ('a', 7654943586),\n",
       " ('is', 4192081707),\n",
       " ('that', 4000373255),\n",
       " ('for', 3272628244),\n",
       " ('it', 2870028984),\n",
       " ('as', 2850302594),\n",
       " ('was', 2751347404),\n",
       " ('with', 2591390604),\n",
       " ('be', 2409421528),\n",
       " ('by', 2351529575),\n",
       " ('on', 2297245630),\n",
       " ('not', 2261359962),\n",
       " ('he', 2055218371),\n",
       " ('i', 1942306760),\n",
       " ('this', 1913024043),\n",
       " ('are', 1850210733),\n",
       " ('or', 1833848095),\n",
       " ('his', 1805683788),\n",
       " ('from', 1734598284),\n",
       " ('at', 1706718058),\n",
       " ('which', 1570109342),\n",
       " ('but', 1396171439),\n",
       " ('have', 1388716420),\n",
       " ('an', 1363116521),\n",
       " ('had', 1308007510),\n",
       " ('they', 1231059987),\n",
       " ('you', 1168867586),\n",
       " ('were', 1135241275),\n",
       " ('their', 1076488415),\n",
       " ('one', 1074487479),\n",
       " ('all', 1031379950),\n",
       " ('we', 1028643905),\n",
       " ('can', 832705210),\n",
       " ('her', 816638617),\n",
       " ('has', 816070042),\n",
       " ('there', 811848203),\n",
       " ('been', 809520620),\n",
       " ('if', 782488523),\n",
       " ('more', 773623990),\n",
       " ('when', 759867879),\n",
       " ('will', 742569455),\n",
       " ('would', 736404476),\n",
       " ('who', 732299236),\n",
       " ('so', 724571145),\n",
       " ('no', 700307542)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_compositions(n):\n",
    "    \"\"\"Yield all sequences of positive integers that sum to n, if n >= 1 then the first item is always [n]\"\"\"\n",
    "    if n < 1:\n",
    "        yield []\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for sub_composition in integer_compositions(i):\n",
    "                yield sub_composition + [n - i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78096290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substring_compositions(s):\n",
    "    \"\"\"Yield all sequences of substring that concatenate to s. If len(s) >= 1 then the first item is always [s]\"\"\"\n",
    "    if not len(s):\n",
    "        yield []\n",
    "    for i in range(len(s)):\n",
    "        for substring in all_substring_compositions(s[:i]):\n",
    "            yield substring + [s[i:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774e56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"Generate a counter with all length n substrings of strings in the input counter\"\"\"\n",
    "    counts = collections.Counter()\n",
    "    for s, count in tqdm(data.items(), desc=f\"Collecting {n}-grams\"):\n",
    "        for i in range(len(s) - n + 1):\n",
    "            counts[s[i:i + n]] += count\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47a71af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_up_to(data, max_n):\n",
    "    \"\"\"Generate a list of counters with all length 1-maxn substrings of strings in the input counter\"\"\"\n",
    "    counts = [collections.Counter() for i in range(max_n)]\n",
    "    for s, count in tqdm(data.items(), desc=f\"Collecting (1..{max_n})-grams\"):\n",
    "        for j in range(1, max_n + 1):\n",
    "            local_counts = counts[j - 1]\n",
    "            for i in range(len(s) - j + 1):\n",
    "                local_counts[s[i:i + j]] += count\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eac3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_ngrams_up_to(data, max_n):\n",
    "    all_ngrams = ngrams_up_to(data, max_n)\n",
    "    all_totals = [sum(ngrams.values()) for ngrams in all_ngrams]\n",
    "    \n",
    "    for n in range(2, max_n + 1):\n",
    "        current_ngrams = all_ngrams[n - 1]\n",
    "        current_total = all_totals[n - 1]\n",
    "        \n",
    "        current_ngrams_to_keep = collections.Counter()\n",
    "        current_ngram_count = len(current_ngrams)\n",
    "        \n",
    "        for _ in trange(len(current_ngrams), desc=f\"Filtering {n}-tuples\"):\n",
    "            ngram, count = current_ngrams.popitem()\n",
    "            ngram_probability = count / current_total\n",
    "            composition_probability = 0  # How probable is it that we find replicate this ngram as a combination of shorter ones\n",
    "            for composition in itertools.islice(all_substring_compositions(ngram), 1, None):\n",
    "                this_composition_probability = 1\n",
    "                try:\n",
    "                    for substring in composition:\n",
    "                        substring_probability = all_ngrams[len(substring) - 1][substring] / all_totals[len(substring) - 1]\n",
    "                        this_composition_probability *= substring_probability\n",
    "                except KeyError:  # The substring of current ngram might not be in the list if it was previously pruned\n",
    "                    pass\n",
    "                else:\n",
    "                    composition_probability += this_composition_probability\n",
    "                    if composition_probability > ngram_probability:\n",
    "                        break\n",
    "\n",
    "            if ngram_probability > composition_probability:\n",
    "                current_ngrams_to_keep[ngram] = count\n",
    "\n",
    "        print(f\"Keeping {len(current_ngrams_to_keep)} ({100 * len(current_ngrams_to_keep) / current_ngram_count:.0f}%) {n}-grams\")\n",
    "        all_ngrams[n - 1] = current_ngrams_to_keep\n",
    "    \n",
    "    for n in reversed(range(2, max_n + 1)):\n",
    "        for i, sub_ngrams in enumerate(ngrams_up_to(all_ngrams[n - 1], n - 1)):\n",
    "            all_ngrams[i] -= sub_ngrams\n",
    "    \n",
    "    return [(ngrams, total) for ngrams, total in zip(all_ngrams, all_totals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b4dd9ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ae7faf87134a919380011d265f1a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting (1..2)-grams:   0%|          | 0/4999714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbf25bfb7304afaaabd69113a4d866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering 2-tuples:   0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 181 (27%) 2-grams\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f3462de5614f23863054df0d66675d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting (1..1)-grams:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s': 0.6%\n",
      "'y': 0.5%\n",
      "'f': 0.2%\n",
      "'w': 0.1%\n",
      "'th': 3.5%\n",
      "'he': 3.0%\n",
      "'in': 2.4%\n",
      "'er': 2.0%\n",
      "'an': 2.0%\n",
      "'re': 1.8%\n",
      "'on': 1.7%\n",
      "'at': 1.5%\n",
      "'en': 1.5%\n",
      "'ti': 1.3%\n",
      "'es': 1.3%\n",
      "'nd': 1.3%\n",
      "'or': 1.3%\n",
      "'te': 1.2%\n",
      "'ed': 1.2%\n",
      "'of': 1.1%\n",
      "'is': 1.1%\n",
      "'it': 1.1%\n",
      "'al': 1.1%\n",
      "'ar': 1.1%\n",
      "'st': 1.1%\n",
      "'nt': 1.0%\n",
      "'to': 1.0%\n",
      "'ng': 0.9%\n",
      "'se': 0.9%\n",
      "'ha': 0.9%\n",
      "'as': 0.9%\n",
      "'ou': 0.9%\n",
      "'le': 0.8%\n",
      "'io': 0.8%\n",
      "'ve': 0.8%\n",
      "'co': 0.8%\n",
      "'me': 0.8%\n",
      "'de': 0.8%\n",
      "'hi': 0.8%\n",
      "'ri': 0.7%\n",
      "'ro': 0.7%\n",
      "'ic': 0.7%\n",
      "'ra': 0.7%\n",
      "'ce': 0.6%\n",
      "'li': 0.6%\n",
      "'ch': 0.6%\n",
      "'ll': 0.6%\n",
      "'ma': 0.6%\n",
      "'be': 0.6%\n",
      "'si': 0.5%\n",
      "'om': 0.5%\n",
      "'ur': 0.5%\n",
      "'ca': 0.5%\n",
      "'el': 0.5%\n",
      "'la': 0.5%\n",
      "'ns': 0.5%\n",
      "'di': 0.5%\n",
      "'ho': 0.5%\n",
      "'fo': 0.5%\n",
      "'pe': 0.5%\n",
      "'ec': 0.5%\n",
      "'pr': 0.5%\n",
      "'us': 0.5%\n",
      "'ct': 0.5%\n",
      "'ac': 0.4%\n",
      "'il': 0.4%\n",
      "'ly': 0.4%\n",
      "'nc': 0.4%\n",
      "'ut': 0.4%\n",
      "'un': 0.4%\n",
      "'lo': 0.4%\n",
      "'ge': 0.4%\n",
      "'wa': 0.4%\n",
      "'em': 0.4%\n",
      "'ol': 0.4%\n",
      "'wh': 0.4%\n",
      "'ad': 0.4%\n",
      "'wi': 0.4%\n",
      "'po': 0.4%\n",
      "'we': 0.4%\n",
      "'ul': 0.3%\n",
      "'mo': 0.3%\n",
      "'pa': 0.3%\n",
      "'ow': 0.3%\n",
      "'mi': 0.3%\n",
      "'im': 0.3%\n",
      "'su': 0.3%\n",
      "'id': 0.3%\n",
      "'am': 0.3%\n",
      "'iv': 0.3%\n",
      "'fi': 0.3%\n",
      "'ci': 0.3%\n",
      "'vi': 0.3%\n",
      "'pl': 0.3%\n",
      "'tu': 0.3%\n",
      "'ig': 0.3%\n",
      "'ev': 0.3%\n",
      "'ld': 0.2%\n",
      "'ry': 0.2%\n",
      "'mp': 0.2%\n",
      "'bl': 0.2%\n",
      "'ab': 0.2%\n",
      "'op': 0.2%\n",
      "'ty': 0.2%\n",
      "'gh': 0.2%\n",
      "'wo': 0.2%\n",
      "'ke': 0.2%\n",
      "'ay': 0.2%\n",
      "'ex': 0.2%\n",
      "'fr': 0.2%\n",
      "'ag': 0.2%\n",
      "'ap': 0.2%\n",
      "'av': 0.2%\n",
      "'if': 0.2%\n",
      "'bo': 0.2%\n",
      "'gr': 0.2%\n",
      "'sp': 0.2%\n",
      "'uc': 0.2%\n",
      "'bu': 0.2%\n",
      "'ov': 0.2%\n",
      "'rm': 0.2%\n",
      "'by': 0.2%\n",
      "'cu': 0.2%\n",
      "'gi': 0.2%\n",
      "'ba': 0.2%\n",
      "'ga': 0.2%\n",
      "'cl': 0.1%\n",
      "'qu': 0.1%\n",
      "'du': 0.1%\n",
      "'yo': 0.1%\n",
      "'ff': 0.1%\n",
      "'um': 0.1%\n",
      "'va': 0.1%\n",
      "'lu': 0.1%\n",
      "'pp': 0.1%\n",
      "'up': 0.1%\n",
      "'ug': 0.1%\n",
      "'ck': 0.1%\n",
      "'mu': 0.1%\n",
      "'br': 0.1%\n",
      "'ak': 0.1%\n",
      "'pu': 0.1%\n",
      "'ki': 0.1%\n",
      "'mm': 0.1%\n",
      "'rk': 0.1%\n",
      "'fu': 0.1%\n",
      "'mb': 0.1%\n",
      "'ub': 0.1%\n",
      "'gu': 0.1%\n",
      "'rv': 0.1%\n",
      "'iz': 0.1%\n",
      "'xp': 0.1%\n",
      "'ok': 0.1%\n",
      "'my': 0.1%\n",
      "'ju': 0.1%\n",
      "'eq': 0.1%\n",
      "'jo': 0.1%\n",
      "'ze': 0.1%\n",
      "'nk': 0.1%\n",
      "'je': 0.1%\n",
      "'kn': 0.1%\n",
      "'ks': 0.0%\n",
      "'xt': 0.0%\n",
      "'ik': 0.0%\n",
      "'sk': 0.0%\n",
      "'xi': 0.0%\n",
      "'xa': 0.0%\n",
      "'ja': 0.0%\n",
      "'za': 0.0%\n",
      "'xc': 0.0%\n",
      "'bj': 0.0%\n",
      "'ix': 0.0%\n",
      "'ox': 0.0%\n",
      "'ax': 0.0%\n",
      "'zi': 0.0%\n",
      "'az': 0.0%\n",
      "'iq': 0.0%\n",
      "'zo': 0.0%\n",
      "'cq': 0.0%\n",
      "'zz': 0.0%\n",
      "'xx': 0.0%\n",
      "'zu': 0.0%\n",
      "'zy': 0.0%\n",
      "'yz': 0.0%\n",
      "'xq': 0.0%\n"
     ]
    }
   ],
   "source": [
    "for ngrams, total in filtered_ngrams_up_to(english_words, 2):\n",
    "    for ngram, count in ngrams.most_common():\n",
    "        print(f\"{ngram!r}: {100 * count / total:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-narrative",
   "metadata": {},
   "source": [
    "# Analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smaller-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(x):\n",
    "    try:\n",
    "        mapped = map(chr, x)\n",
    "    except TypeError:\n",
    "        return repr(chr(x))\n",
    "    else:\n",
    "        return repr(\"\".join(mapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e53cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(iterable, key=lambda x: x):\n",
    "    it =  iter(iterable)\n",
    "    try:\n",
    "        min_v = next(it)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    max_v = min_v\n",
    "    min_k = key(min_v)\n",
    "    max_k = min_k\n",
    "    \n",
    "    for v in it:\n",
    "        k = key(v)\n",
    "        if k < min_k:\n",
    "            min_v = v\n",
    "            min_k = k\n",
    "        if k > max_k:\n",
    "            max_v = v\n",
    "            max_k = k\n",
    "    \n",
    "    return min_v, max_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd327242",
   "metadata": {},
   "source": [
    "## Variable length Huffman coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hydraulic-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_match_length(s, mask):\n",
    "    for mask_length, limited_mask in mask.items():\n",
    "        if tuple(s[:mask_length]) in limited_mask:\n",
    "            #print(s, \"masked by\", s[:mask_length])\n",
    "            return mask_length\n",
    "    return None\n",
    "\n",
    "def n_tuples_masked(data, n, mask):\n",
    "    ret = collections.Counter()\n",
    "    for s, count in data.items():\n",
    "        i = 0\n",
    "        i_limit = len(s) - n + 1\n",
    "        while i < i_limit:\n",
    "            mask_match = mask_match_length(s[i:], mask)\n",
    "            if mask_match is not None:\n",
    "                i += mask_match\n",
    "            else:\n",
    "                ret[tuple(s[i:i + n])] += count\n",
    "                i += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f30ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixes(x):\n",
    "    \"\"\"Return a set with all prefixes of the string (or tuple or something similar), not including empty string and x itself\"\"\"\n",
    "    return set(x[:i] for i in range(1, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb11f9fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_symbols(data, max_length, max_length_count, require_full_trie):\n",
    "    \"\"\"Generate symbol tuples up to max_length long with their corresponding probabilities based on data (in a counter).\n",
    "    This takes into account that at each point of the input we try to encode the next bytes using the longest\n",
    "    available symbol first.\n",
    "    \n",
    "    Also we force all prefixes of longer tuples to be included in the output to avoid empty spaces in the encoding trie.\n",
    "    \"\"\"\n",
    "    symbols = {}\n",
    "    mask_symbols = {}\n",
    "    required_tuples = set()\n",
    "        \n",
    "    # The loop is unrolled for i == max_length\n",
    "    # and i in range(1, max_length)\n",
    "    \n",
    "    \n",
    "    print(f\"Generating length {max_length} symbols\")\n",
    "    all_tuples = n_tuples(data, max_length)\n",
    "    total_count = sum(all_tuples.values())\n",
    "    local_symbols = {}\n",
    "    for t, count in all_tuples.most_common(max_length_count):\n",
    "        probability = count / total_count\n",
    "        local_symbols[t] = probability\n",
    "        required_tuples |= prefixes(t)\n",
    "        worst_symbols_per_bit = probability * max_length\n",
    "            # TODO: Probabilty does not drive the huffman code length here,\n",
    "            # because we're not considering all tuples...\n",
    "    \n",
    "    symbols.update(local_symbols)\n",
    "    mask_symbols[max_length] = local_symbols\n",
    "    print(f\"    Got {len(local_symbols)} symbols\")\n",
    "    \n",
    "    for i in reversed(range(1, max_length)):\n",
    "        print(f\"Generating length {i} symbols\")\n",
    "        \n",
    "        # Filter the n-tuples of given length to not include instances already covered\n",
    "        # by the longer tuples when using greedy encoding\n",
    "        all_tuples = n_tuples_masked(data, i, mask_symbols)\n",
    "\n",
    "        total_count = sum(all_tuples.values())\n",
    "        local_symbols = {}\n",
    "        \n",
    "        if require_full_trie:\n",
    "            # First insert all tuples required by longer lengths\n",
    "            for t in (all_tuples.keys() & required_tuples):\n",
    "                probability = count / total_count\n",
    "                local_symbols[t] = probability\n",
    "                required_tuples |= prefixes(t)\n",
    "\n",
    "            print(f\"    {len(local_symbols)} symbols required\")\n",
    "        \n",
    "        # Then start inserting from the highest probabilty tuples\n",
    "        heap = [(-count, t) for t, count in all_tuples.items()]\n",
    "        heapq.heapify(heap)\n",
    "        while True:\n",
    "            minus_count, t = heapq.heappop(heap)\n",
    "            count = -minus_count\n",
    "            probability = count / total_count\n",
    "            symbols_per_bit = probability * i\n",
    "            if symbols_per_bit < worst_symbols_per_bit:\n",
    "                break\n",
    "            \n",
    "            local_symbols[t] = probability\n",
    "            #required_tuples |= prefixes(t)\n",
    "\n",
    "        symbols.update(local_symbols)\n",
    "        mask_symbols[i] = local_symbols\n",
    "        print(f\"    Got {len(local_symbols)} symbols\")\n",
    "        \n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a43c0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_tree(symbols):\n",
    "    \"\"\"Build a huffman tree from a dict of symbol: probability. \"\"\"\n",
    "    \n",
    "    heap = [(x[1], True, x[0]) for x in symbols.items()]\n",
    "    # Heap elements look like: probability, leaf flag, (leaf data | children).\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        first = heapq.heappop(heap)\n",
    "        second = heap[0]\n",
    "        \n",
    "        if first > second:\n",
    "            (first, second) = (second, first)\n",
    "        \n",
    "        new = (first[0] + second[0], False, (first, second))\n",
    "        \n",
    "        heapq.heapreplace(heap, new)\n",
    "        \n",
    "    encoding = {}\n",
    "        \n",
    "    def flatten_tree(element, current_encoding=\"\"):\n",
    "        if element[1]:  # is leaf\n",
    "            encoding[element[2]] = current_encoding\n",
    "        else:\n",
    "            flatten_tree(element[2][0], current_encoding + \"0\")\n",
    "            flatten_tree(element[2][1], current_encoding + \"1\")\n",
    "    \n",
    "    flatten_tree(heap[0])\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4701d0bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating length 4 symbols\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Counter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m symbols \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_symbols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire_full_trie\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mgenerate_symbols\u001b[0;34m(data, max_length, max_length_count, require_full_trie)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# The loop is unrolled for i == max_length\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# and i in range(1, max_length)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m symbols\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m all_tuples \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(all_tuples\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     19\u001b[0m local_symbols \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Counter' object is not callable"
     ]
    }
   ],
   "source": [
    "symbols = generate_symbols(data, max_length=4, max_length_count=5, require_full_trie=False)\n",
    "#symbols = generate_symbols(data, max_length=3, max_length_count=30, require_full_trie=False)\n",
    "#symbols = generate_symbols(data, max_length=3, max_length_count=50, require_full_trie=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15606482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned_huffman_tree(symbols, max_bits, require_full_trie, max_items=float(\"inf\")):\n",
    "    all_symbols = symbols\n",
    "    symbols = all_symbols.copy()\n",
    "    \n",
    "    while True:\n",
    "        encoding = huffman_tree(symbols)\n",
    "                \n",
    "        max_bits_used = max(len(x) for x in encoding.values())\n",
    "        \n",
    "        required_symbols = set()\n",
    "        if require_full_trie:\n",
    "            for symbol in symbols:\n",
    "                required_symbols |= prefixes(symbol)\n",
    "            \n",
    "        def expected_shortening(x):\n",
    "            \"\"\"Return expected decrease of size of the output if we include this item\"\"\"\n",
    "            return (8 * len(x) - len(encoding[x])) * symbols[x]\n",
    "        \n",
    "        def victim_key(x):\n",
    "            \"\"\"Use probability of the symbol as a secondary key\"\"\"\n",
    "            return (expected_shortening(x), symbols[x]) \n",
    "\n",
    "        worst_sym, best_sym = minmax(encoding.keys(), key=victim_key)\n",
    "        \n",
    "        def format_symbol(x):\n",
    "            return f\"{b(x)} ({len(encoding[x])}b, p = {100 * symbols[x]:.2f}%, CR = {100 * len(encoding[x]) / (8 * len(x)):.2f}%, expected shortening = {expected_shortening(x):.3f}b)\"\n",
    "        \n",
    "        print(f\"Encoded {len(encoding)} symbols, max code length {max_bits_used}, {len(required_symbols)} required symbols\")\n",
    "        print(\"  Best symbol\", format_symbol(best_sym))\n",
    "        print(\"  Worst symbol\", format_symbol(worst_sym))\n",
    "        \n",
    "        victim = min(symbols.keys() - required_symbols, key=victim_key)\n",
    "        \n",
    "        if \\\n",
    "            expected_shortening(victim) > 0 and \\\n",
    "            max_bits_used <= max_bits and \\\n",
    "            len(encoding) <= max_items:\n",
    "            return encoding\n",
    "\n",
    "        print(\"Removing symbol\", format_symbol(victim))\n",
    "        \n",
    "        del symbols[victim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63851f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = pruned_huffman_tree(symbols, 12, require_full_trie=False)\n",
    "#encoding = huffman_tree(symbols)\n",
    "print(len(encoding))\n",
    "for tup, enc in sorted(encoding.items(), key=lambda x: (symbols[x[0]]), reverse=True):\n",
    "    print(f\"{b(tup)} -> {enc} ({len(enc)}b, p = {100 * symbols[tup]:.2f}%, expected_shortening = {symbols[tup] * (8 * len(tup) - len(enc)):.3f}b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa074a1f",
   "metadata": {},
   "source": [
    "Make sure that the encoding is actually a prefix code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80863ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup1, enc1 in encoding.items():\n",
    "    for tup2, enc2 in encoding.items():\n",
    "        if tup1 == tup2:\n",
    "            continue\n",
    "        if enc1.startswith(enc2):\n",
    "            print(f\"{b(tup1)} -> {enc1} starts with {b(tup2)} -> {enc2}\")\n",
    "        assert not enc1.startswith(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dict = {\"\".join(chr(x) for x in key).encode(\"ascii\"): e for key, e in encoding.items()}\n",
    "encoding_dict_key_length = max(len(key) for key in encoding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ebe03",
   "metadata": {},
   "source": [
    "### Encoding using a trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0691a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(encoding):\n",
    "    trie = {}\n",
    "    for tup, enc in sorted(encoding.items(), key=lambda x: x[0]):\n",
    "        current = trie\n",
    "        for c in tup[:-1]:\n",
    "            current = current.setdefault(c, (None, {}))[1]\n",
    "        if tup[-1] in current:\n",
    "            assert current[tup[-1]][0] is None\n",
    "            current[tup[-1]] = (enc, current[tup[-1]][1])\n",
    "        else:\n",
    "            current[tup[-1]] = (enc, {})\n",
    "    \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trie(trie, indent=\"\"):\n",
    "    for c, (enc, children) in trie.items():\n",
    "        print(f\"{indent}{b(c)}: {enc}\")\n",
    "        print_trie(children, indent + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660f2e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trie = build_trie(encoding)\n",
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cffbfc",
   "metadata": {},
   "source": [
    "### Encoding using perfect hashing\n",
    "This uses ideas from gperf (https://www.dre.vanderbilt.edu/~schmidt/PDF/gperf.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_perfect_hash(encoding):\n",
    "    min_c, max_c = minmax(c for tup in encoding.keys() for c in tup)\n",
    "    \n",
    "    print(\"total key characters\", sum(len(key) for key in encoding.keys()))\n",
    "    \n",
    "    key_lengths = collections.Counter(len(key) for key in encoding.keys())\n",
    "    print(key_lengths)\n",
    "    \n",
    "    print(sum(key_lengths.values()))\n",
    "    \n",
    "    asso_size = max_c - min_c + 2  # + extra item for out of range\n",
    "    print(max_c - min_c)\n",
    "    asso = [0] * asso_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_perfect_hash(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19050679",
   "metadata": {},
   "outputs": [],
   "source": [
    "72**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-plaza",
   "metadata": {},
   "source": [
    "## Some predictor ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_error_size(e):\n",
    "    \"\"\" Return size in bits of given error encoded using tablog's stream encoder, assuming perfect adaptation \"\"\"\n",
    "    if e == 0:\n",
    "         return 1\n",
    "    else:\n",
    "         return 2 + 1 + math.floor(1 + math.log2(abs(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852db0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_number_size(n):\n",
    "    return 2 * math.floor(math.log2(n + 1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb26bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictor(method):\n",
    "    print(f\"Evaluating {method.__name__}\")\n",
    "    strings = 0\n",
    "    log_bits_per_char_sum = 0\n",
    "    log_bits_per_char_unique_sum = 0\n",
    "    for s, count in tqdm(data.items()):\n",
    "        string_bits = method(s)\n",
    "        bits_per_char = string_bits / len(s)\n",
    "        \n",
    "        log_bits_per_char_sum += count * math.log(string_bits / len(s))\n",
    "        log_bits_per_char_unique_sum += math.log(string_bits / len(s))\n",
    "        \n",
    "        strings += count\n",
    "\n",
    "    print(f\"    geometric mean {math.exp(log_bits_per_char_sum / strings):.2f} bits/char\")\n",
    "    print(f\"    geometric mean (unique) {math.exp(log_bits_per_char_unique_sum / len(data)):.2f} bits/char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_predictor(s):\n",
    "    ord_e = ord(\"e\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        sum(encoded_error_size(c - ord_e) for c in s)\n",
    "evaluate_predictor(e_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_char_predictor(s):\n",
    "    first_prediction = ord(\"e\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        encoded_error_size(s[0] - first_prediction) + \\\n",
    "        sum(encoded_error_size(c - prev) for c, prev in zip(s[1:], s[:-1]))\n",
    "evaluate_predictor(same_char_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_predictor(s):\n",
    "    two_c = 215\n",
    "    first_prediction = ord(\"s\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        encoded_error_size(s[0] - first_prediction) + \\\n",
    "        sum(encoded_error_size(c - (two_c - prev)) for c, prev in zip(s[1:], s[:-1]))\n",
    "evaluate_predictor(reflection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_style(s):\n",
    "    return 8 + 8 * len(s)\n",
    "evaluate_predictor(c_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_freq_table(s):\n",
    "    #freq_table = b\"etisarnodlc_puf\"\n",
    "    freq_table = b\"etisarnodlc_pufmbghyvxTwkERICSz\"\n",
    "    \n",
    "    ret = encoded_number_size(len(s))\n",
    "    \n",
    "    for c in s:\n",
    "        try:\n",
    "            index = freq_table.index(c)\n",
    "        except ValueError:\n",
    "            ret += 1 + 8\n",
    "        else:\n",
    "            ret += encoded_number_size(index + 1)\n",
    "            \n",
    "    return ret\n",
    "evaluate_predictor(small_freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_verb_lengths(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    verbatim_length = 0\n",
    "    verbatim_size = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            if verbatim_length > 0:\n",
    "                encoded_length = 1 + encoded_number_size(verbatim_length - 1) + verbatim_size\n",
    "                if printing:\n",
    "                    print(f\"{verbatim_length} verbatim characters: {encoded_length}b, {encoded_length/verbatim_length:.2f}b/char\")\n",
    "                ret += encoded_length\n",
    "                verbatim_length = 0\n",
    "                verbatim_size = 0\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            i += 1\n",
    "            verbatim_length += 1\n",
    "            verbatim_size += 8\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "            \n",
    "    if verbatim_length > 0:\n",
    "        encoded_length = 1 + encoded_number_size(verbatim_length - 1) + verbatim_size\n",
    "        if printing:\n",
    "            print(f\"(flush) {verbatim_length} verbatim characters: {encoded_length}b, {encoded_length/verbatim_length:.2f}b/char\")\n",
    "        ret += 1 + encoded_number_size(verbatim_length) + verbatim_size\n",
    "        verbatim_length = 0\n",
    "        verbatim_size = 0\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_verb_lengths)\n",
    "#Evaluating huffman_dict_verb_lengths\n",
    "#    geometric mean 6.99 bits/char\n",
    "#    geometric mean (unique) 6.57 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc4bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def huffman_dict(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            encoded_length = 1 + 8\n",
    "            if printing:\n",
    "                print(f\"{b(s[i])} verbatim: {encoded_length}b\")\n",
    "            ret += encoded_length\n",
    "            i += 1\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict)\n",
    "#Evaluating huffman_dict\n",
    "#    geometric mean 6.95 bits/char\n",
    "#    geometric mean (unique) 6.52 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_diff_verb(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    i = 0\n",
    "    previous = ord('e')\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            previous = s[i + j - 1]\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            encoded_length = 1\n",
    "            error = s[i] - previous\n",
    "            if error == 0:\n",
    "                encoded_length += 1\n",
    "            else:\n",
    "                abs_error = abs(error) - 1\n",
    "                fixed_bits = 4\n",
    "                encoded_length += encoded_number_size(abs_error >> fixed_bits) + fixed_bits\n",
    "            if printing:\n",
    "                print(f\"{b(s[i])} not in dict: {encoded_length}b\")\n",
    "            ret += encoded_length\n",
    "            previous = s[i]\n",
    "            i += 1\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_diff_verb)\n",
    "#Evaluating huffman_dict_diff_verb\n",
    "#    geometric mean 6.90 bits/char\n",
    "#    geometric mean (unique) 6.40 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9690268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_grouped_hits(s, printing=False):\n",
    "    global encoding_dict, encoding_dict_key_length\n",
    "    \n",
    "    def match_at(i):\n",
    "        \"\"\"Return number of characters of the string at position i that match the dictionary and the encoded length\"\"\"\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            else:\n",
    "                return j, len(encoded)\n",
    "        return (0, None)\n",
    "    \n",
    "    def count_matches(i):\n",
    "        \"\"\"Return number of successive dictionary matches/mismatches starting at i and whether we finished at the end of the string\"\"\"\n",
    "        count = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if match_length:\n",
    "                count += 1\n",
    "                i += match_length\n",
    "            else:\n",
    "                return count, False\n",
    "        return count, True\n",
    "    \n",
    "    def count_mismatches(i):\n",
    "        \"\"\"Return number of successive dictionary matches/mismatches starting at i and whether we finished at the end of the string\"\"\"\n",
    "        count = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if not match_length:\n",
    "                count += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                return count, False\n",
    "        return count, True\n",
    "    \n",
    "    def encoded_matches(i):\n",
    "        length = 0\n",
    "        encoded_size = 0\n",
    "        while i < len(s):\n",
    "            match_length, match_encoded_size = match_at(i)\n",
    "            if match_length:\n",
    "                if printing:\n",
    "                    print(f\"  {s[i:i+match_length]}: {match_encoded_size}b, {match_encoded_size/match_length:.2f}b/char\")\n",
    "                i += match_length\n",
    "                length += match_length\n",
    "                encoded_size += match_encoded_size\n",
    "            else:\n",
    "                break\n",
    "        return length, encoded_size\n",
    "    \n",
    "    def encoded_mismatches(i, prev):\n",
    "        length = 0\n",
    "        encoded_size = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if not match_length:\n",
    "                error = s[i] - prev\n",
    "                prev = s[i]\n",
    "                if error == 0:\n",
    "                    e = 1\n",
    "                else:\n",
    "                    abs_error = abs(error) - 1\n",
    "                    fixed_bits = 3\n",
    "                    e = encoded_number_size(abs_error >> fixed_bits) + fixed_bits\n",
    "                \n",
    "                if printing:\n",
    "                    print(f\"  {bytes([s[i]])}: {e}b\")\n",
    "                i += 1\n",
    "                length += 1\n",
    "                encoded_size += e\n",
    "            else:\n",
    "                break\n",
    "        return length, encoded_size\n",
    "    \n",
    "    ret = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Handle a first block separately, we allow zero number of matches in it\n",
    "    block_length, end = count_matches(i)\n",
    "    if printing:\n",
    "        print(f\"First match block: {block_length}\")\n",
    "    ret += encoded_number_size(block_length)\n",
    "        # Might be zero, if:\n",
    "        # a) Starting with a non-match character\n",
    "        # b) Processing an empty string\n",
    "    length, encoded_size = encoded_matches(i)\n",
    "    i += length\n",
    "    ret += encoded_size\n",
    "    \n",
    "    if i > 0:\n",
    "        prev = s[i - 1]\n",
    "    else:\n",
    "        prev = ord(\"e\")\n",
    "    \n",
    "    while True:\n",
    "        block_length, end = count_mismatches(i)\n",
    "        if printing:\n",
    "            print(f\"Mismatch block: {block_length}\")\n",
    "        assert block_length > 0 or end\n",
    "        ret += encoded_number_size(block_length)\n",
    "        if not block_length:\n",
    "            break\n",
    "        length, encoded_size = encoded_mismatches(i, prev)\n",
    "        i += length\n",
    "        ret += encoded_size\n",
    "        \n",
    "        block_length, end = count_matches(i)\n",
    "        if printing:\n",
    "            print(f\"Match block: {block_length}\")\n",
    "        assert block_length > 0 or end\n",
    "        ret += encoded_number_size(block_length)\n",
    "        if not block_length:\n",
    "            break\n",
    "        length, encoded_size = encoded_matches(i)\n",
    "        i += length\n",
    "        ret += encoded_size\n",
    "        prev = s[i - 1]\n",
    "    \n",
    "    if printing:\n",
    "        if len(s) > 0:\n",
    "            print(f\"{ret}b, {ret / len(s):.2f} bits/char\")\n",
    "        else:\n",
    "            print(f\"{ret}b\")\n",
    "            \n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_grouped_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60a346",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_log_bits_per_char = 0\n",
    "count = 0\n",
    "for s in manual_test_data:\n",
    "    print(s)\n",
    "    bits = huffman_dict_grouped_hits(s, printing=True)\n",
    "    if bits > 0 and len(s) > 0:\n",
    "        sum_log_bits_per_char += math.log(bits / len(s))\n",
    "        count += 1\n",
    "    print()\n",
    "    \n",
    "print(f\"Geometric mean {math.exp(sum_log_bits_per_char / count):.2f}b/char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a52ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaz_dict = [\"\\002s,\\266\", \"\\003had\\232\\002leW\", \"\\003on \\216\", \"\", \"\\001yS\",\n",
    "\"\\002ma\\255\\002li\\227\", \"\\003or \\260\", \"\", \"\\002ll\\230\\003s t\\277\",\n",
    "\"\\004fromg\\002mel\", \"\", \"\\003its\\332\", \"\\001z\\333\", \"\\003ingF\", \"\\001>\\336\",\n",
    "\"\\001 \\000\\003   (\\002nc\\344\", \"\\002nd=\\003 on\\312\",\n",
    "\"\\002ne\\213\\003hat\\276\\003re q\", \"\", \"\\002ngT\\003herz\\004have\\306\\003s o\\225\",\n",
    "\"\", \"\\003ionk\\003s a\\254\\002ly\\352\", \"\\003hisL\\003 inN\\003 be\\252\", \"\",\n",
    "\"\\003 fo\\325\\003 of \\003 ha\\311\", \"\", \"\\002of\\005\",\n",
    "\"\\003 co\\241\\002no\\267\\003 ma\\370\", \"\", \"\", \"\\003 cl\\356\\003enta\\003 an7\",\n",
    "\"\\002ns\\300\\001\\\"e\", \"\\003n t\\217\\002ntP\\003s, \\205\",\n",
    "\"\\002pe\\320\\003 we\\351\\002om\\223\", \"\\002on\\037\", \"\", \"\\002y G\", \"\\003 wa\\271\",\n",
    "\"\\003 re\\321\\002or*\", \"\", \"\\002=\\\"\\251\\002ot\\337\", \"\\003forD\\002ou[\",\n",
    "\"\\003 toR\", \"\\003 th\\r\", \"\\003 it\\366\",\n",
    "\"\\003but\\261\\002ra\\202\\003 wi\\363\\002</\\361\", \"\\003 wh\\237\", \"\\002  4\",\n",
    "\"\\003nd ?\", \"\\002re!\", \"\", \"\\003ng c\", \"\",\n",
    "\"\\003ly \\307\\003ass\\323\\001a\\004\\002rir\", \"\", \"\", \"\", \"\\002se_\", \"\\003of \\\"\",\n",
    "\"\\003div\\364\\002ros\\003ere\\240\", \"\", \"\\002ta\\310\\001bZ\\002si\\324\", \"\",\n",
    "\"\\003and\\a\\002rs\\335\", \"\\002rt\\362\", \"\\002teE\", \"\\003ati\\316\", \"\\002so\\263\",\n",
    "\"\\002th\\021\", \"\\002tiJ\\001c\\034\\003allp\", \"\\003ate\\345\", \"\\002ss\\246\",\n",
    "\"\\002stM\", \"\", \"\\002><\\346\", \"\\002to\\024\", \"\\003arew\", \"\\001d\\030\",\n",
    "\"\\002tr\\303\", \"\", \"\\001\\n1\\003 a \\222\", \"\\003f tv\\002veo\", \"\\002un\\340\", \"\",\n",
    "\"\\003e o\\242\", \"\\002a \\243\\002wa\\326\\001e\\002\", \"\\002ur\\226\\003e a\\274\",\n",
    "\"\\002us\\244\\003\\n\\r\\n\\247\", \"\\002ut\\304\\003e c\\373\", \"\\002we\\221\", \"\", \"\",\n",
    "\"\\002wh\\302\", \"\\001f,\", \"\", \"\", \"\", \"\\003d t\\206\", \"\", \"\", \"\\003th \\343\",\n",
    "\"\\001g;\", \"\", \"\", \"\\001\\r9\\003e s\\265\", \"\\003e t\\234\", \"\", \"\\003to Y\",\n",
    "\"\\003e\\r\\n\\236\", \"\\002d \\036\\001h\\022\", \"\", \"\\001,Q\", \"\\002 a\\031\", \"\\002 b^\",\n",
    "\"\\002\\r\\n\\025\\002 cI\", \"\\002 d\\245\", \"\\002 e\\253\", \"\\002 fh\\001i\\b\\002e \\v\",\n",
    "\"\", \"\\002 hU\\001-\\314\", \"\\002 i8\", \"\", \"\", \"\\002 l\\315\", \"\\002 m{\",\n",
    "\"\\002f :\\002 n\\354\", \"\\002 o\\035\", \"\\002 p}\\001.n\\003\\r\\n\\r\\250\", \"\",\n",
    "\"\\002 r\\275\", \"\\002 s>\", \"\\002 t\\016\", \"\", \"\\002g \\235\\005which+\\003whi\\367\",\n",
    "\"\\002 w5\", \"\\001/\\305\", \"\\003as \\214\", \"\\003at \\207\", \"\", \"\\003who\\331\", \"\",\n",
    "\"\\001l\\026\\002h \\212\", \"\", \"\\002, $\", \"\", \"\\004withV\", \"\", \"\", \"\", \"\\001m-\", \"\",\n",
    "\"\", \"\\002ac\\357\", \"\\002ad\\350\", \"\\003TheH\", \"\", \"\", \"\\004this\\233\\001n\\t\",\n",
    "\"\", \"\\002. y\", \"\", \"\\002alX\\003e, \\365\", \"\\003tio\\215\\002be\\\\\",\n",
    "\"\\002an\\032\\003ver\\347\", \"\", \"\\004that0\\003tha\\313\\001o\\006\", \"\\003was2\",\n",
    "\"\\002arO\", \"\\002as.\", \"\\002at'\\003the\\001\\004they\\200\\005there\\322\\005theird\",\n",
    "\"\\002ce\\210\", \"\\004were]\", \"\", \"\\002ch\\231\\002l \\264\\001p<\", \"\", \"\",\n",
    "\"\\003one\\256\", \"\", \"\\003he \\023\\002dej\", \"\\003ter\\270\", \"\\002cou\", \"\",\n",
    "\"\\002by\\177\\002di\\201\\002eax\", \"\", \"\\002ec\\327\", \"\\002edB\", \"\\002ee\\353\", \"\",\n",
    "\"\", \"\\001r\\f\\002n )\", \"\", \"\", \"\", \"\\002el\\262\", \"\", \"\\003in i\\002en3\", \"\",\n",
    "\"\\002o `\\001s\\n\", \"\", \"\\002er\\033\", \"\\003is t\\002es6\", \"\", \"\\002ge\\371\",\n",
    "\"\\004.com\\375\", \"\\002fo\\334\\003our\\330\", \"\\003ch \\301\\001t\\003\", \"\\002hab\", \"\",\n",
    "\"\\003men\\374\", \"\", \"\\002he\\020\", \"\", \"\", \"\\001u&\", \"\\002hif\", \"\",\n",
    "\"\\003not\\204\\002ic\\203\", \"\\003ed @\\002id\\355\", \"\", \"\", \"\\002ho\\273\",\n",
    "\"\\002r K\\001vm\", \"\", \"\", \"\", \"\\003t t\\257\\002il\\360\", \"\\002im\\342\",\n",
    "\"\\003en \\317\\002in\\017\", \"\\002io\\220\", \"\\002s \\027\\001wA\", \"\", \"\\003er |\",\n",
    "\"\\003es ~\\002is%\", \"\\002it/\", \"\", \"\\002iv\\272\", \"\",\n",
    "\"\\002t #\\ahttp://C\\001x\\372\", \"\\002la\\211\", \"\\001<\\341\", \"\\003, a\\224\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(x) + 1 for x in smaz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "241 * 3 + 141*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3753624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
