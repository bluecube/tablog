{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-softball",
   "metadata": {},
   "source": [
    "# String predictor\n",
    "\n",
    "Tablog encodes column name strings in headers and we want to compress them at least a bit -- the strings should not be the dominant part of the dataset, but still saving few bytes is always good.\n",
    "\n",
    "This file contains experiments that should help determining an appropriate compression algorithm.\n",
    "\n",
    "Goals for this part are (in order of importance):\n",
    "1. fast and light\n",
    "2. simple-ishto implement\n",
    "3. compresses well enough to make me happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86d5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "import heapq\n",
    "import itertools\n",
    "import string\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d5d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17054480",
   "metadata": {},
   "source": [
    "## Manual test data\n",
    "These are just a couple of strings that I feel will be representative of the intended use of Tablog.\n",
    "Used mostly to get an intuitive feel of how the compression behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test_data = [\n",
    "    \"deadbeef\", \"hello_world\", \"voltage\", \"current\", \"left\", \"column58\", \"left_motor_speed\", \"kP\", \"right_motor_kD\",\n",
    "    \"POWER!\", \"timestamp\", \"AverageTicks\", \"temperature\", \"MCU-power\", \"alpha\", \"beta\", \"omega\", \"velocityX\",\n",
    "    \"!@#$%^&*()\", \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-parking",
   "metadata": {},
   "source": [
    "## Harvesting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc6b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_from_example_directories(regex):\n",
    "    \"\"\"Recursively go through all source-looking files in the\n",
    "    directories and return a counter of words matching regex.\"\"\"\n",
    "    \n",
    "    extensions = (\".cpp\", \".hpp\", \".c\", \".h\", \".py\", \".sh\")\n",
    "    directories = [\n",
    "        \"/home/cube/development/c/tablog/encoder\",\n",
    "        \"/home/cube/development/c/tablog/decoder\",\n",
    "        \"/home/cube/development/c/tablog/integration_tests\",\n",
    "        \"/usr/src/linux\"\n",
    "    ]\n",
    "\n",
    "    regex = re.compile(regex)\n",
    "    \n",
    "    ret = collections.Counter()\n",
    "    \n",
    "    for directory in tqdm(directories, desc=\"Looking through directories\"):\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if not file.endswith(extensions):\n",
    "                    continue\n",
    "                path = os.path.join(root, file)\n",
    "                with open(path, \"r\") as fp:\n",
    "                    for line in fp:\n",
    "                        for match in regex.finditer(line):\n",
    "                            ret[match.group(0)] += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-academy",
   "metadata": {},
   "source": [
    "Load strings that look like identifiers from Tablog sources and  from Linux kernel sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0c246a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = regex_from_example_directories(r\"(?<![a-zA-Z0-9_])[a-zA-Z_][a-zA-Z0-9_]*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba44f6",
   "metadata": {},
   "source": [
    "## English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b27a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_books_1grams_words():\n",
    "    ret = collections.Counter()\n",
    "    good_string_re = re.compile(r\"^([a-zA-Z]*)\\t[^\\t]*\\t([0-9]*)\")\n",
    "    for letter in tqdm(string.ascii_lowercase):\n",
    "        r = requests.get(f\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-{letter}.gz\", stream=True)\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=file_size, desc=letter) as compressed_response_file:\n",
    "            with gzip.open(compressed_response_file, \"rt\") as response_file:\n",
    "                for line in response_file:\n",
    "                    if match := good_string_re.match(line):\n",
    "                        ret[match[1]] += int(match[2])\n",
    "                        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf35224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbc874f978844e78068ae053e369889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd4afa3e48c4871a5abb96eb425a870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a:   0%|          | 0/343487895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08769465cef74b6b9fc57c2fd2592f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b:   0%|          | 0/247964820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d9d5ffce354908a49f16cabef56c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c:   0%|          | 0/393712861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd24c0eac3c47de9dd895d1c4763d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "d:   0%|          | 0/239389478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe0fcce5c6143c5a74ae2f66bc6f7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "e:   0%|          | 0/204996476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4590e10e23374035ae438004a15aa9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "f:   0%|          | 0/185305580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957429d6398b48fe9ad85b536ed87bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "g:   0%|          | 0/160767731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f17ca420fef4a6bbbdedfe22051aac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "h:   0%|          | 0/183438682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4616b65b65492080f7a6cd797d3678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "i:   0%|          | 0/203553215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76962d7d17164413af0960becc84ed24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/64839808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409a03f79b8d46c389b0679e7e7d0cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "k:   0%|          | 0/109307817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8549d2cca514982b1824b0da94457cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "l:   0%|          | 0/188008415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfcec9ef828410ca5191ac288f7c22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "m:   0%|          | 0/289750659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde33811df9e4b38ba5112e4a32f35e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n:   0%|          | 0/141931087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b4c39f494d4309b4d7171bf53134cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "o:   0%|          | 0/139934943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b02cbb0ce09430f88d503a5bfef9244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "p:   0%|          | 0/357808228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24eb16627d1d49059851d9511f9213ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "q:   0%|          | 0/26608642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e01bbe044445c4a1477219adfadc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r:   0%|          | 0/216873218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5e08e07fc34876be8dc67d3527e1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "s:   0%|          | 0/444315601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1daddf55dad40a887b1233ce9fa1b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "t:   0%|          | 0/265071292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bbcffa32f24834a206f6693d6dfa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "u:   0%|          | 0/88124330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeb9acf553149f697d9b85f1c05bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v:   0%|          | 0/108848534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd94263b94fc44fb94fc8e8034cb1385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "w:   0%|          | 0/118835349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95daa552574341c8a38438315e576d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "x:   0%|          | 0/14649720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8b2813c11346f7862b73d2786e4059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "y:   0%|          | 0/26595711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ba9b1aa6cf4cd3ae9d5abfd68e7970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "z:   0%|          | 0/27324007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_words = google_books_1grams_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8da69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23688414489),\n",
       " ('of', 15342397280),\n",
       " ('and', 11021132912),\n",
       " ('to', 9494905988),\n",
       " ('in', 7611765281),\n",
       " ('a', 7083003595),\n",
       " ('is', 4139526351),\n",
       " ('that', 3870260345),\n",
       " ('for', 3021925527),\n",
       " ('The', 2763139452),\n",
       " ('was', 2737725870),\n",
       " ('as', 2626386982),\n",
       " ('with', 2504225400),\n",
       " ('be', 2392576396),\n",
       " ('by', 2249978492),\n",
       " ('not', 2208497726),\n",
       " ('it', 2204134503),\n",
       " ('on', 2143313303),\n",
       " ('I', 1873234887),\n",
       " ('are', 1826889845),\n",
       " ('or', 1805149769),\n",
       " ('his', 1672173489),\n",
       " ('from', 1651527506),\n",
       " ('at', 1562321315),\n",
       " ('which', 1557215562),\n",
       " ('he', 1529423270),\n",
       " ('this', 1423199366),\n",
       " ('have', 1372997478),\n",
       " ('had', 1298386780),\n",
       " ('an', 1266190915)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87233c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substring_compositions(s):\n",
    "    \"\"\"Yield all sequences of substring that concatenate to s. If len(s) >= 1 then the first item is always [s]\"\"\"\n",
    "    if not len(s):\n",
    "        yield []\n",
    "    for i in range(len(s)):\n",
    "        for substring in all_substring_compositions(s[:i]):\n",
    "            yield substring + [s[i:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55623fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"Generate a counter with all length n substrings of strings in the input counter\"\"\"\n",
    "    counts = collections.Counter()\n",
    "    for s, count in data.items():\n",
    "        for i in range(len(s) - n + 1):\n",
    "            counts[s[i:i + n]] += count\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b742b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_up_to(data, max_n):\n",
    "    \"\"\"Generate a list of counters with all length 1-maxn substrings of strings in the input counter\"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for s, count in tqdm(data.items(), desc=f\"Collecting (1..{max_n})-grams\"):\n",
    "        for j in range(1, max_n + 1):\n",
    "            for i in range(len(s) - j + 1):\n",
    "                ret[s[i:i + j]] += count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "111ef2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f1191071864f648fd129cfc9d62fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting (1..5)-grams:   0%|          | 0/6857277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_words_ngrams = ngrams_up_to(english_words, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e068",
   "metadata": {},
   "source": [
    "## Dictionary extras\n",
    "Padding the dictionary with ngrams by stuff that we expect to appear in the inputs, but which is not contained in the english words dataset.\n",
    "\n",
    "The weights of different symbols are manually estimated and may be significantly non-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518b387",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ec76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numbers_ngrams(p, max_value, max_ngram_length):\n",
    "    ret = collections.Counter()\n",
    "    for i in range(max_value):\n",
    "        weight = p * (1 - p)**i\n",
    "        s = str(i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    for i in range(int(math.log2(max_value))):\n",
    "        weight =  p * (1 - p)**i\n",
    "        s = str(2**i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    for i in range(int(math.log10(max_value))):\n",
    "        weight = 2 * p * (1 - p)**i\n",
    "        s = str(10**i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef6e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_ngrams = generate_numbers_ngrams(p=0.25, max_value=10000, max_ngram_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55baca3",
   "metadata": {},
   "source": [
    "### Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ee839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ngrams(*weighted_strings):\n",
    "    \"\"\" Generate a length-separated list with ngrams directly provided. Argument is (weight, ngram) tuples. \"\"\"\n",
    "    ret = []\n",
    "    for weight, ngram in weighted_strings:\n",
    "        while len(ret) <= len(ngram):\n",
    "            ret.append(collections.Counter())\n",
    "        \n",
    "        ret[len(ngram)][ngram] += weight\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8434e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_ngrams = collections.Counter([\n",
    "    (\"_\", 1), (\" \", .5), (\"-\", .2),\n",
    "    (\".\", .2), (.5, \",\"), (\", \", .2), (\". \", .2), (.1, \",\\n\"), (.2, \";\\n\"),\n",
    "    (\"http\", .1), (\"@\", .05)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cecdd2",
   "metadata": {},
   "source": [
    "### Capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc518de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ngrams(ngrams, func):\n",
    "    \"\"\" Map all keys of ngrams \"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for ngram, count in tqdm(ngrams.items()):\n",
    "        ret[func(ngram)] += count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba549d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb7c6bc4ecb48b7b0f996d42279e4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3266865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caps_english_words_ngrams = map_ngrams(english_words_ngrams, lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5e0c1",
   "metadata": {},
   "source": [
    "## Merging ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e94dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ngrams(*weighted_ngrams):\n",
    "    \"\"\" Merge tuples of (weight, ngrams) to a single ngram block.\n",
    "    Weights within each ngram block are normalized first, so that they sum up to 1. \"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for weight, ngrams in tqdm(weighted_ngrams):\n",
    "        factor = weight / sum(ngrams.values())\n",
    "\n",
    "        for ngram, count in tqdm(ngrams.items()):\n",
    "            ret[ngram] += factor * count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e862d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ngrams(data):\n",
    "    total_count = 0\n",
    "    for ngrams, total in data:\n",
    "        for ngram, count in ngrams.most_common():\n",
    "            total_count += 1\n",
    "            print(f\"{ngram!r}: {100 * count / total:.1f}%\")\n",
    "    print(f\"Total {total_count} different ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2225e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf0ff4a55df40259fa6422d32c3ffa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdc5c3dca5a465a884c3122f8fa07f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3266865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491f630682ca422db65d354877c07217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1657513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce4d9c4c60d4f50af88b460afa898f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127e7719ec9142f6a938a1d410495181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_ngrams = merge_ngrams(\n",
    "    (1, english_words_ngrams),\n",
    "    (0.02, caps_english_words_ngrams),\n",
    "    (0.02, number_ngrams),\n",
    "    (0.05, symbols_ngrams)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5291fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_counter_by_length(counter):\n",
    "    ret = []\n",
    "    for s, count in tqdm(counter.items(), desc=\"Splitting by length\"):\n",
    "        while len(s) > len(ret):\n",
    "            ret.append(collections.Counter())\n",
    "        \n",
    "        ret[len(s) - 1][s] += count\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e1f8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ngrams(input_ngrams, min_goodness=0, max_n=float(\"inf\")):\n",
    "    def ngram_goodness(ngram_length, ngram_probability):\n",
    "        \"\"\"Return an estimate of how much this ngram will help with making the encoding smaller.\"\"\"\n",
    "        # TODO: This is based just on my feeling on how the filtering should behave...\n",
    "        encoded_length = -math.log2(ngram_probability)\n",
    "        return ngram_probability * ngram_length / encoded_length\n",
    "    \n",
    "    max_n = min(max_n, max(len(key) for key in input_ngrams.keys()))\n",
    "    total = sum(count for count in input_ngrams.values())\n",
    "    \n",
    "    split = split_counter_by_length(input_ngrams)\n",
    "    \n",
    "    for n in trange(2, max_n + 1):\n",
    "        current_ngrams = split[n - 1]\n",
    "        current_ngrams_to_keep = collections.Counter()\n",
    "        current_ngram_count = len(current_ngrams)\n",
    "                \n",
    "        for ngram, count in tqdm(current_ngrams.items(), desc=f\"Filtering {n}-tuples\"):\n",
    "            ngram_probability = count / total\n",
    "            if ngram_goodness(n, ngram_probability) < min_goodness:\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            # Find shorter ngrams that would be combined to create this one\n",
    "            i = 0\n",
    "            alternative_ngrams = []\n",
    "            while i < n:\n",
    "                for j in reversed(range(1, n)):\n",
    "                    if ngram[i:i+j] in split[j]:\n",
    "                        alternative_ngrams.append(ngram[i:i+j])\n",
    "                        i += j\n",
    "                        break\n",
    "                else:\n",
    "                    i += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            composition_probability = 0  # How probable is it that we find replicate this ngram as a combination of shorter ones\n",
    "            for composition in itertools.islice(all_substring_compositions(ngram), 1, None):\n",
    "                this_composition_probability = 1\n",
    "                try:\n",
    "                    for substring in composition:\n",
    "                        substring_probability = split[len(substring) - 1][substring] / totals[len(substring) - 1]\n",
    "                        this_composition_probability *= substring_probability\n",
    "                except KeyError:  # The substring of current ngram might not be in the list if it was previously pruned\n",
    "                    pass\n",
    "                else:\n",
    "                    composition_probability += this_composition_probability\n",
    "                    if composition_probability > ngram_probability:\n",
    "                        break\n",
    "\n",
    "            if ngram_probability > composition_probability:\n",
    "                current_ngrams_to_keep[ngram] = count\n",
    "\n",
    "        print(f\"Keeping {len(current_ngrams_to_keep)} ({100 * len(current_ngrams_to_keep) / current_ngram_count:.0f}%) {n}-grams\")\n",
    "        split[n] = current_ngrams_to_keep\n",
    "    \n",
    "        for sub_ngram, sub_ngram_count in ngrams_up_to(split[n - 1], n - 1).items():\n",
    "            split[len(sub_ngram) - 1][sub_ngram] -= sub_ngram_count\n",
    "    \n",
    "    for i, (current_ngrams, current_total) in enumerate(zip(filtered_ngrams, totals)):\n",
    "        for ngram, count in current_ngrams.items():\n",
    "            ngram_probability = count / current_total\n",
    "            if ngram_goodness(n, ngram_probability) < min_goodness:\n",
    "                continue\n",
    "    \n",
    "    return [(ngrams, total) for ngrams, total in zip(filtered_ngrams, totals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b261c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a781469c6b7475c8b000b04d28bab5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting by length:   0%|          | 0/4421739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21be4a74627c4d1bbe2a6e203d7f5c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a817bde47b9d46d0a8b96bd4be512104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering 2-tuples:   0%|          | 0/2815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 0 (0%) 2-grams\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ed1cfed87a46fbbf0afd99fc3bb507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting (1..1)-grams:   0%|          | 0/2815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ceef835b7a4dbaa125aefd500b0c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering 3-tuples: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_merged_ngrams \u001b[38;5;241m=\u001b[39m filter_ngrams(merged_ngrams, min_goodness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mfilter_ngrams\u001b[0;34m(input_ngrams, min_goodness, max_n)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngram_probability \u001b[38;5;241m>\u001b[39m composition_probability:\n\u001b[1;32m     54\u001b[0m         current_ngrams_to_keep[ngram] \u001b[38;5;241m=\u001b[39m count\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeeping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(current_ngrams_to_keep)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_ngrams_to_keep) \u001b[38;5;241m/\u001b[39m current_ngram_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-grams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m split[n] \u001b[38;5;241m=\u001b[39m current_ngrams_to_keep\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_ngram, sub_ngram_count \u001b[38;5;129;01min\u001b[39;00m ngrams_up_to(split[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "filtered_merged_ngrams = filter_ngrams(merged_ngrams, min_goodness=0.01, max_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c39254",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_ngrams(filtered_merged_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-narrative",
   "metadata": {},
   "source": [
    "# Analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(iterable, key=lambda x: x):\n",
    "    it =  iter(iterable)\n",
    "    try:\n",
    "        min_v = next(it)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    max_v = min_v\n",
    "    min_k = key(min_v)\n",
    "    max_k = min_k\n",
    "    \n",
    "    for v in it:\n",
    "        k = key(v)\n",
    "        if k < min_k:\n",
    "            min_v = v\n",
    "            min_k = k\n",
    "        if k > max_k:\n",
    "            max_v = v\n",
    "            max_k = k\n",
    "    \n",
    "    return min_v, max_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53a561",
   "metadata": {},
   "source": [
    "## Variable length Huffman coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c606a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_tree(symbols):\n",
    "    \"\"\"Build a huffman tree from a dict of symbol: probability. \"\"\"\n",
    "    \n",
    "    heap = [(x[1], True, x[0]) for x in symbols.items()]\n",
    "    # Heap elements look like: probability, leaf flag, (leaf data | children).\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        first = heapq.heappop(heap)\n",
    "        second = heap[0]\n",
    "        \n",
    "        if first > second:\n",
    "            (first, second) = (second, first)\n",
    "        \n",
    "        new = (first[0] + second[0], False, (first, second))\n",
    "        \n",
    "        heapq.heapreplace(heap, new)\n",
    "        \n",
    "    encoding = {}\n",
    "        \n",
    "    def flatten_tree(element, current_encoding=\"\"):\n",
    "        if element[1]:  # is leaf\n",
    "            encoding[element[2]] = current_encoding\n",
    "        else:\n",
    "            flatten_tree(element[2][0], current_encoding + \"0\")\n",
    "            flatten_tree(element[2][1], current_encoding + \"1\")\n",
    "    \n",
    "    flatten_tree(heap[0])\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2eee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned_huffman_tree(symbols, max_bits, require_full_trie, max_items=float(\"inf\")):\n",
    "    all_symbols = symbols\n",
    "    symbols = all_symbols.copy()\n",
    "    \n",
    "    while True:\n",
    "        encoding = huffman_tree(symbols)\n",
    "                \n",
    "        max_bits_used = max(len(x) for x in encoding.values())\n",
    "        \n",
    "        required_symbols = set()\n",
    "        if require_full_trie:\n",
    "            for symbol in symbols:\n",
    "                required_symbols |= prefixes(symbol)\n",
    "            \n",
    "        def expected_shortening(x):\n",
    "            \"\"\"Return expected decrease of size of the output if we include this item\"\"\"\n",
    "            return (8 * len(x) - len(encoding[x])) * symbols[x]\n",
    "        \n",
    "        def victim_key(x):\n",
    "            \"\"\"Use probability of the symbol as a secondary key\"\"\"\n",
    "            return (expected_shortening(x), symbols[x]) \n",
    "\n",
    "        worst_sym, best_sym = minmax(encoding.keys(), key=victim_key)\n",
    "        \n",
    "        def format_symbol(x):\n",
    "            return f\"{b(x)} ({len(encoding[x])}b, p = {100 * symbols[x]:.2f}%, CR = {100 * len(encoding[x]) / (8 * len(x)):.2f}%, expected shortening = {expected_shortening(x):.3f}b)\"\n",
    "        \n",
    "        print(f\"Encoded {len(encoding)} symbols, max code length {max_bits_used}, {len(required_symbols)} required symbols\")\n",
    "        print(\"  Best symbol\", format_symbol(best_sym))\n",
    "        print(\"  Worst symbol\", format_symbol(worst_sym))\n",
    "        \n",
    "        victim = min(symbols.keys() - required_symbols, key=victim_key)\n",
    "        \n",
    "        if \\\n",
    "            expected_shortening(victim) > 0 and \\\n",
    "            max_bits_used <= max_bits and \\\n",
    "            len(encoding) <= max_items:\n",
    "            return encoding\n",
    "\n",
    "        print(\"Removing symbol\", format_symbol(victim))\n",
    "        \n",
    "        del symbols[victim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = pruned_huffman_tree(symbols, 12, require_full_trie=False)\n",
    "#encoding = huffman_tree(symbols)\n",
    "print(len(encoding))\n",
    "for tup, enc in sorted(encoding.items(), key=lambda x: (symbols[x[0]]), reverse=True):\n",
    "    print(f\"{b(tup)} -> {enc} ({len(enc)}b, p = {100 * symbols[tup]:.2f}%, expected_shortening = {symbols[tup] * (8 * len(tup) - len(enc)):.3f}b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee73153",
   "metadata": {},
   "source": [
    "Make sure that the encoding is actually a prefix code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup1, enc1 in encoding.items():\n",
    "    for tup2, enc2 in encoding.items():\n",
    "        if tup1 == tup2:\n",
    "            continue\n",
    "        if enc1.startswith(enc2):\n",
    "            print(f\"{b(tup1)} -> {enc1} starts with {b(tup2)} -> {enc2}\")\n",
    "        assert not enc1.startswith(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece29032",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dict = {\"\".join(chr(x) for x in key).encode(\"ascii\"): e for key, e in encoding.items()}\n",
    "encoding_dict_key_length = max(len(key) for key in encoding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07e3f",
   "metadata": {},
   "source": [
    "### Encoding using a trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(encoding):\n",
    "    trie = {}\n",
    "    for tup, enc in sorted(encoding.items(), key=lambda x: x[0]):\n",
    "        current = trie\n",
    "        for c in tup[:-1]:\n",
    "            current = current.setdefault(c, (None, {}))[1]\n",
    "        if tup[-1] in current:\n",
    "            assert current[tup[-1]][0] is None\n",
    "            current[tup[-1]] = (enc, current[tup[-1]][1])\n",
    "        else:\n",
    "            current[tup[-1]] = (enc, {})\n",
    "    \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trie(trie, indent=\"\"):\n",
    "    for c, (enc, children) in trie.items():\n",
    "        print(f\"{indent}{b(c)}: {enc}\")\n",
    "        print_trie(children, indent + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c55105",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trie = build_trie(encoding)\n",
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ff13e",
   "metadata": {},
   "source": [
    "### Encoding using perfect hashing\n",
    "This uses ideas from gperf (https://www.dre.vanderbilt.edu/~schmidt/PDF/gperf.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca49f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_perfect_hash(encoding):\n",
    "    min_c, max_c = minmax(c for tup in encoding.keys() for c in tup)\n",
    "    \n",
    "    print(\"total key characters\", sum(len(key) for key in encoding.keys()))\n",
    "    \n",
    "    key_lengths = collections.Counter(len(key) for key in encoding.keys())\n",
    "    print(key_lengths)\n",
    "    \n",
    "    print(sum(key_lengths.values()))\n",
    "    \n",
    "    asso_size = max_c - min_c + 2  # + extra item for out of range\n",
    "    print(max_c - min_c)\n",
    "    asso = [0] * asso_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_perfect_hash(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "72**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-plaza",
   "metadata": {},
   "source": [
    "## Some predictor ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ab24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_error_size(e):\n",
    "    \"\"\" Return size in bits of given error encoded using tablog's stream encoder, assuming perfect adaptation \"\"\"\n",
    "    if e == 0:\n",
    "         return 1\n",
    "    else:\n",
    "         return 2 + 1 + math.floor(1 + math.log2(abs(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_number_size(n):\n",
    "    return 2 * math.floor(math.log2(n + 1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b87974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictor(method):\n",
    "    print(f\"Evaluating {method.__name__}\")\n",
    "    strings = 0\n",
    "    log_bits_per_char_sum = 0\n",
    "    log_bits_per_char_unique_sum = 0\n",
    "    for s, count in tqdm(data.items()):\n",
    "        string_bits = method(s)\n",
    "        bits_per_char = string_bits / len(s)\n",
    "        \n",
    "        log_bits_per_char_sum += count * math.log(string_bits / len(s))\n",
    "        log_bits_per_char_unique_sum += math.log(string_bits / len(s))\n",
    "        \n",
    "        strings += count\n",
    "\n",
    "    print(f\"    geometric mean {math.exp(log_bits_per_char_sum / strings):.2f} bits/char\")\n",
    "    print(f\"    geometric mean (unique) {math.exp(log_bits_per_char_unique_sum / len(data)):.2f} bits/char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_predictor(s):\n",
    "    ord_e = ord(\"e\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        sum(encoded_error_size(c - ord_e) for c in s)\n",
    "evaluate_predictor(e_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_char_predictor(s):\n",
    "    first_prediction = ord(\"e\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        encoded_error_size(s[0] - first_prediction) + \\\n",
    "        sum(encoded_error_size(c - prev) for c, prev in zip(s[1:], s[:-1]))\n",
    "evaluate_predictor(same_char_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_predictor(s):\n",
    "    two_c = 215\n",
    "    first_prediction = ord(\"s\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        encoded_error_size(s[0] - first_prediction) + \\\n",
    "        sum(encoded_error_size(c - (two_c - prev)) for c, prev in zip(s[1:], s[:-1]))\n",
    "evaluate_predictor(reflection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_style(s):\n",
    "    return 8 + 8 * len(s)\n",
    "evaluate_predictor(c_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_freq_table(s):\n",
    "    #freq_table = b\"etisarnodlc_puf\"\n",
    "    freq_table = b\"etisarnodlc_pufmbghyvxTwkERICSz\"\n",
    "    \n",
    "    ret = encoded_number_size(len(s))\n",
    "    \n",
    "    for c in s:\n",
    "        try:\n",
    "            index = freq_table.index(c)\n",
    "        except ValueError:\n",
    "            ret += 1 + 8\n",
    "        else:\n",
    "            ret += encoded_number_size(index + 1)\n",
    "            \n",
    "    return ret\n",
    "evaluate_predictor(small_freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_verb_lengths(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    verbatim_length = 0\n",
    "    verbatim_size = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            if verbatim_length > 0:\n",
    "                encoded_length = 1 + encoded_number_size(verbatim_length - 1) + verbatim_size\n",
    "                if printing:\n",
    "                    print(f\"{verbatim_length} verbatim characters: {encoded_length}b, {encoded_length/verbatim_length:.2f}b/char\")\n",
    "                ret += encoded_length\n",
    "                verbatim_length = 0\n",
    "                verbatim_size = 0\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            i += 1\n",
    "            verbatim_length += 1\n",
    "            verbatim_size += 8\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "            \n",
    "    if verbatim_length > 0:\n",
    "        encoded_length = 1 + encoded_number_size(verbatim_length - 1) + verbatim_size\n",
    "        if printing:\n",
    "            print(f\"(flush) {verbatim_length} verbatim characters: {encoded_length}b, {encoded_length/verbatim_length:.2f}b/char\")\n",
    "        ret += 1 + encoded_number_size(verbatim_length) + verbatim_size\n",
    "        verbatim_length = 0\n",
    "        verbatim_size = 0\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_verb_lengths)\n",
    "#Evaluating huffman_dict_verb_lengths\n",
    "#    geometric mean 6.99 bits/char\n",
    "#    geometric mean (unique) 6.57 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20809e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def huffman_dict(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            encoded_length = 1 + 8\n",
    "            if printing:\n",
    "                print(f\"{b(s[i])} verbatim: {encoded_length}b\")\n",
    "            ret += encoded_length\n",
    "            i += 1\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict)\n",
    "#Evaluating huffman_dict\n",
    "#    geometric mean 6.95 bits/char\n",
    "#    geometric mean (unique) 6.52 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a050335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_diff_verb(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    i = 0\n",
    "    previous = ord('e')\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            previous = s[i + j - 1]\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            encoded_length = 1\n",
    "            error = s[i] - previous\n",
    "            if error == 0:\n",
    "                encoded_length += 1\n",
    "            else:\n",
    "                abs_error = abs(error) - 1\n",
    "                fixed_bits = 4\n",
    "                encoded_length += encoded_number_size(abs_error >> fixed_bits) + fixed_bits\n",
    "            if printing:\n",
    "                print(f\"{b(s[i])} not in dict: {encoded_length}b\")\n",
    "            ret += encoded_length\n",
    "            previous = s[i]\n",
    "            i += 1\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_diff_verb)\n",
    "#Evaluating huffman_dict_diff_verb\n",
    "#    geometric mean 6.90 bits/char\n",
    "#    geometric mean (unique) 6.40 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5473c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_grouped_hits(s, printing=False):\n",
    "    global encoding_dict, encoding_dict_key_length\n",
    "    \n",
    "    def match_at(i):\n",
    "        \"\"\"Return number of characters of the string at position i that match the dictionary and the encoded length\"\"\"\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            else:\n",
    "                return j, len(encoded)\n",
    "        return (0, None)\n",
    "    \n",
    "    def count_matches(i):\n",
    "        \"\"\"Return number of successive dictionary matches/mismatches starting at i and whether we finished at the end of the string\"\"\"\n",
    "        count = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if match_length:\n",
    "                count += 1\n",
    "                i += match_length\n",
    "            else:\n",
    "                return count, False\n",
    "        return count, True\n",
    "    \n",
    "    def count_mismatches(i):\n",
    "        \"\"\"Return number of successive dictionary matches/mismatches starting at i and whether we finished at the end of the string\"\"\"\n",
    "        count = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if not match_length:\n",
    "                count += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                return count, False\n",
    "        return count, True\n",
    "    \n",
    "    def encoded_matches(i):\n",
    "        length = 0\n",
    "        encoded_size = 0\n",
    "        while i < len(s):\n",
    "            match_length, match_encoded_size = match_at(i)\n",
    "            if match_length:\n",
    "                if printing:\n",
    "                    print(f\"  {s[i:i+match_length]}: {match_encoded_size}b, {match_encoded_size/match_length:.2f}b/char\")\n",
    "                i += match_length\n",
    "                length += match_length\n",
    "                encoded_size += match_encoded_size\n",
    "            else:\n",
    "                break\n",
    "        return length, encoded_size\n",
    "    \n",
    "    def encoded_mismatches(i, prev):\n",
    "        length = 0\n",
    "        encoded_size = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if not match_length:\n",
    "                error = s[i] - prev\n",
    "                prev = s[i]\n",
    "                if error == 0:\n",
    "                    e = 1\n",
    "                else:\n",
    "                    abs_error = abs(error) - 1\n",
    "                    fixed_bits = 3\n",
    "                    e = encoded_number_size(abs_error >> fixed_bits) + fixed_bits\n",
    "                \n",
    "                if printing:\n",
    "                    print(f\"  {bytes([s[i]])}: {e}b\")\n",
    "                i += 1\n",
    "                length += 1\n",
    "                encoded_size += e\n",
    "            else:\n",
    "                break\n",
    "        return length, encoded_size\n",
    "    \n",
    "    ret = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Handle a first block separately, we allow zero number of matches in it\n",
    "    block_length, end = count_matches(i)\n",
    "    if printing:\n",
    "        print(f\"First match block: {block_length}\")\n",
    "    ret += encoded_number_size(block_length)\n",
    "        # Might be zero, if:\n",
    "        # a) Starting with a non-match character\n",
    "        # b) Processing an empty string\n",
    "    length, encoded_size = encoded_matches(i)\n",
    "    i += length\n",
    "    ret += encoded_size\n",
    "    \n",
    "    if i > 0:\n",
    "        prev = s[i - 1]\n",
    "    else:\n",
    "        prev = ord(\"e\")\n",
    "    \n",
    "    while True:\n",
    "        block_length, end = count_mismatches(i)\n",
    "        if printing:\n",
    "            print(f\"Mismatch block: {block_length}\")\n",
    "        assert block_length > 0 or end\n",
    "        ret += encoded_number_size(block_length)\n",
    "        if not block_length:\n",
    "            break\n",
    "        length, encoded_size = encoded_mismatches(i, prev)\n",
    "        i += length\n",
    "        ret += encoded_size\n",
    "        \n",
    "        block_length, end = count_matches(i)\n",
    "        if printing:\n",
    "            print(f\"Match block: {block_length}\")\n",
    "        assert block_length > 0 or end\n",
    "        ret += encoded_number_size(block_length)\n",
    "        if not block_length:\n",
    "            break\n",
    "        length, encoded_size = encoded_matches(i)\n",
    "        i += length\n",
    "        ret += encoded_size\n",
    "        prev = s[i - 1]\n",
    "    \n",
    "    if printing:\n",
    "        if len(s) > 0:\n",
    "            print(f\"{ret}b, {ret / len(s):.2f} bits/char\")\n",
    "        else:\n",
    "            print(f\"{ret}b\")\n",
    "            \n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_grouped_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfa378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_log_bits_per_char = 0\n",
    "count = 0\n",
    "for s in manual_test_data:\n",
    "    print(s)\n",
    "    bits = huffman_dict_grouped_hits(s, printing=True)\n",
    "    if bits > 0 and len(s) > 0:\n",
    "        sum_log_bits_per_char += math.log(bits / len(s))\n",
    "        count += 1\n",
    "    print()\n",
    "    \n",
    "print(f\"Geometric mean {math.exp(sum_log_bits_per_char / count):.2f}b/char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a51f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaz_dict = [\"\\002s,\\266\", \"\\003had\\232\\002leW\", \"\\003on \\216\", \"\", \"\\001yS\",\n",
    "\"\\002ma\\255\\002li\\227\", \"\\003or \\260\", \"\", \"\\002ll\\230\\003s t\\277\",\n",
    "\"\\004fromg\\002mel\", \"\", \"\\003its\\332\", \"\\001z\\333\", \"\\003ingF\", \"\\001>\\336\",\n",
    "\"\\001 \\000\\003   (\\002nc\\344\", \"\\002nd=\\003 on\\312\",\n",
    "\"\\002ne\\213\\003hat\\276\\003re q\", \"\", \"\\002ngT\\003herz\\004have\\306\\003s o\\225\",\n",
    "\"\", \"\\003ionk\\003s a\\254\\002ly\\352\", \"\\003hisL\\003 inN\\003 be\\252\", \"\",\n",
    "\"\\003 fo\\325\\003 of \\003 ha\\311\", \"\", \"\\002of\\005\",\n",
    "\"\\003 co\\241\\002no\\267\\003 ma\\370\", \"\", \"\", \"\\003 cl\\356\\003enta\\003 an7\",\n",
    "\"\\002ns\\300\\001\\\"e\", \"\\003n t\\217\\002ntP\\003s, \\205\",\n",
    "\"\\002pe\\320\\003 we\\351\\002om\\223\", \"\\002on\\037\", \"\", \"\\002y G\", \"\\003 wa\\271\",\n",
    "\"\\003 re\\321\\002or*\", \"\", \"\\002=\\\"\\251\\002ot\\337\", \"\\003forD\\002ou[\",\n",
    "\"\\003 toR\", \"\\003 th\\r\", \"\\003 it\\366\",\n",
    "\"\\003but\\261\\002ra\\202\\003 wi\\363\\002</\\361\", \"\\003 wh\\237\", \"\\002  4\",\n",
    "\"\\003nd ?\", \"\\002re!\", \"\", \"\\003ng c\", \"\",\n",
    "\"\\003ly \\307\\003ass\\323\\001a\\004\\002rir\", \"\", \"\", \"\", \"\\002se_\", \"\\003of \\\"\",\n",
    "\"\\003div\\364\\002ros\\003ere\\240\", \"\", \"\\002ta\\310\\001bZ\\002si\\324\", \"\",\n",
    "\"\\003and\\a\\002rs\\335\", \"\\002rt\\362\", \"\\002teE\", \"\\003ati\\316\", \"\\002so\\263\",\n",
    "\"\\002th\\021\", \"\\002tiJ\\001c\\034\\003allp\", \"\\003ate\\345\", \"\\002ss\\246\",\n",
    "\"\\002stM\", \"\", \"\\002><\\346\", \"\\002to\\024\", \"\\003arew\", \"\\001d\\030\",\n",
    "\"\\002tr\\303\", \"\", \"\\001\\n1\\003 a \\222\", \"\\003f tv\\002veo\", \"\\002un\\340\", \"\",\n",
    "\"\\003e o\\242\", \"\\002a \\243\\002wa\\326\\001e\\002\", \"\\002ur\\226\\003e a\\274\",\n",
    "\"\\002us\\244\\003\\n\\r\\n\\247\", \"\\002ut\\304\\003e c\\373\", \"\\002we\\221\", \"\", \"\",\n",
    "\"\\002wh\\302\", \"\\001f,\", \"\", \"\", \"\", \"\\003d t\\206\", \"\", \"\", \"\\003th \\343\",\n",
    "\"\\001g;\", \"\", \"\", \"\\001\\r9\\003e s\\265\", \"\\003e t\\234\", \"\", \"\\003to Y\",\n",
    "\"\\003e\\r\\n\\236\", \"\\002d \\036\\001h\\022\", \"\", \"\\001,Q\", \"\\002 a\\031\", \"\\002 b^\",\n",
    "\"\\002\\r\\n\\025\\002 cI\", \"\\002 d\\245\", \"\\002 e\\253\", \"\\002 fh\\001i\\b\\002e \\v\",\n",
    "\"\", \"\\002 hU\\001-\\314\", \"\\002 i8\", \"\", \"\", \"\\002 l\\315\", \"\\002 m{\",\n",
    "\"\\002f :\\002 n\\354\", \"\\002 o\\035\", \"\\002 p}\\001.n\\003\\r\\n\\r\\250\", \"\",\n",
    "\"\\002 r\\275\", \"\\002 s>\", \"\\002 t\\016\", \"\", \"\\002g \\235\\005which+\\003whi\\367\",\n",
    "\"\\002 w5\", \"\\001/\\305\", \"\\003as \\214\", \"\\003at \\207\", \"\", \"\\003who\\331\", \"\",\n",
    "\"\\001l\\026\\002h \\212\", \"\", \"\\002, $\", \"\", \"\\004withV\", \"\", \"\", \"\", \"\\001m-\", \"\",\n",
    "\"\", \"\\002ac\\357\", \"\\002ad\\350\", \"\\003TheH\", \"\", \"\", \"\\004this\\233\\001n\\t\",\n",
    "\"\", \"\\002. y\", \"\", \"\\002alX\\003e, \\365\", \"\\003tio\\215\\002be\\\\\",\n",
    "\"\\002an\\032\\003ver\\347\", \"\", \"\\004that0\\003tha\\313\\001o\\006\", \"\\003was2\",\n",
    "\"\\002arO\", \"\\002as.\", \"\\002at'\\003the\\001\\004they\\200\\005there\\322\\005theird\",\n",
    "\"\\002ce\\210\", \"\\004were]\", \"\", \"\\002ch\\231\\002l \\264\\001p<\", \"\", \"\",\n",
    "\"\\003one\\256\", \"\", \"\\003he \\023\\002dej\", \"\\003ter\\270\", \"\\002cou\", \"\",\n",
    "\"\\002by\\177\\002di\\201\\002eax\", \"\", \"\\002ec\\327\", \"\\002edB\", \"\\002ee\\353\", \"\",\n",
    "\"\", \"\\001r\\f\\002n )\", \"\", \"\", \"\", \"\\002el\\262\", \"\", \"\\003in i\\002en3\", \"\",\n",
    "\"\\002o `\\001s\\n\", \"\", \"\\002er\\033\", \"\\003is t\\002es6\", \"\", \"\\002ge\\371\",\n",
    "\"\\004.com\\375\", \"\\002fo\\334\\003our\\330\", \"\\003ch \\301\\001t\\003\", \"\\002hab\", \"\",\n",
    "\"\\003men\\374\", \"\", \"\\002he\\020\", \"\", \"\", \"\\001u&\", \"\\002hif\", \"\",\n",
    "\"\\003not\\204\\002ic\\203\", \"\\003ed @\\002id\\355\", \"\", \"\", \"\\002ho\\273\",\n",
    "\"\\002r K\\001vm\", \"\", \"\", \"\", \"\\003t t\\257\\002il\\360\", \"\\002im\\342\",\n",
    "\"\\003en \\317\\002in\\017\", \"\\002io\\220\", \"\\002s \\027\\001wA\", \"\", \"\\003er |\",\n",
    "\"\\003es ~\\002is%\", \"\\002it/\", \"\", \"\\002iv\\272\", \"\",\n",
    "\"\\002t #\\ahttp://C\\001x\\372\", \"\\002la\\211\", \"\\001<\\341\", \"\\003, a\\224\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940719c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(x) + 1 for x in smaz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9226c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "241 * 3 + 141*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
