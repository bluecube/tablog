{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-softball",
   "metadata": {},
   "source": [
    "# String predictor\n",
    "\n",
    "Tablog encodes column name strings in headers and we want to compress them at least a bit -- the strings should not be the dominant part of the dataset, but still saving few bytes is always good.\n",
    "\n",
    "This file contains experiments that should help determining an appropriate compression algorithm.\n",
    "\n",
    "Goals for this part are (in order of importance):\n",
    "1. fast and light\n",
    "2. simple-ish to implement\n",
    "3. compresses well enough to make me happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86d5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "import heapq\n",
    "import itertools\n",
    "import string\n",
    "import gzip\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d5d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17054480",
   "metadata": {},
   "source": [
    "## Manual test data\n",
    "These are just a couple of strings that I feel will be representative of the intended use of Tablog.\n",
    "Used mostly to get an intuitive feel of how the compression behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test_data = [\n",
    "    \"deadbeef\", \"hello_world\", \"voltage\", \"current\", \"left\", \"column58\", \"left_motor_speed\", \"kP\", \"right_motor_kD\",\n",
    "    \"POWER!\", \"timestamp\", \"AverageTicks\", \"temperature\", \"MCU-power\", \"alpha\", \"beta\", \"omega\", \"velocityX\",\n",
    "    \"+-!@#$%^&*()[]{}\", \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba44f6",
   "metadata": {},
   "source": [
    "## English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b27a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_books_1grams_words():\n",
    "    ret = collections.Counter()\n",
    "    good_string_re = re.compile(r\"^([a-zA-Z]*)\\t[^\\t]*\\t([0-9]*)\")\n",
    "    for letter in tqdm(string.ascii_lowercase):\n",
    "        r = requests.get(f\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-{letter}.gz\", stream=True)\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=file_size, desc=letter) as compressed_response_file:\n",
    "            with gzip.open(compressed_response_file, \"rt\") as response_file:\n",
    "                for line in response_file:\n",
    "                    if match := good_string_re.match(line):\n",
    "                        ret[match[1]] += int(match[2])\n",
    "                        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf35224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa346add7ae44e8b1ff48a67952fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f7747e80f34054b0ff22c948c69697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a:   0%|          | 0/343487895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a7e40b878341e0bbda04fd09f0f47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b:   0%|          | 0/247964820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e24353a6b68423496ce045403c6e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c:   0%|          | 0/393712861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20f56353037453c9d247782f549f929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "d:   0%|          | 0/239389478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caae6f4c18ce4d2d8189494453510329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "e:   0%|          | 0/204996476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9362c336b8134cbe88f530116d1b7ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "f:   0%|          | 0/185305580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ac9532bc794fb08720a78370e8e6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "g:   0%|          | 0/160767731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29f33b9f25d4749854b7252529fe6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "h:   0%|          | 0/183438682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b8c9f1e10346e0b7aed8574b03e682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "i:   0%|          | 0/203553215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59222fe6cb4b482f8dc1270a266f5773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/64839808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c09b3e6044391abc80c5b402b8d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "k:   0%|          | 0/109307817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed318ab0b8d04a7c8f98e7d08fcc0204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "l:   0%|          | 0/188008415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda3d6e92e424fe0bbf6ce292cd837aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "m:   0%|          | 0/289750659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9606edca34854f68a8179c462e487dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n:   0%|          | 0/141931087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f564751255f54018a8e77d7cc7fcad33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "o:   0%|          | 0/139934943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1837d87b17a141f4b7a1705fc30300d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "p:   0%|          | 0/357808228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadb6684711c4dfcb21e0c9598a1e599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "q:   0%|          | 0/26608642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbb722b3ee7416cbe67f4b50eca3a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r:   0%|          | 0/216873218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865396d8cc07487fba3c3bab30b79cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "s:   0%|          | 0/444315601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a23eadd6d34e499cf0ec26eb1537f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "t:   0%|          | 0/265071292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61792204ced14989be8c458519f7360f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "u:   0%|          | 0/88124330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eef47d2a2824f4ab3b7221bc0dbf899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v:   0%|          | 0/108848534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedff9ce817d4568b68d68892a658532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "w:   0%|          | 0/118835349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5f4f9476fc45b88501741028519752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "x:   0%|          | 0/14649720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb67cbb85924faa9cbd743489f1c0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "y:   0%|          | 0/26595711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7478552becb47f1917a6a8840fb93ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "z:   0%|          | 0/27324007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_words = google_books_1grams_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc8da69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23688414489),\n",
       " ('of', 15342397280),\n",
       " ('and', 11021132912),\n",
       " ('to', 9494905988),\n",
       " ('in', 7611765281),\n",
       " ('a', 7083003595),\n",
       " ('is', 4139526351),\n",
       " ('that', 3870260345),\n",
       " ('for', 3021925527),\n",
       " ('The', 2763139452),\n",
       " ('was', 2737725870),\n",
       " ('as', 2626386982),\n",
       " ('with', 2504225400),\n",
       " ('be', 2392576396),\n",
       " ('by', 2249978492),\n",
       " ('not', 2208497726),\n",
       " ('it', 2204134503),\n",
       " ('on', 2143313303),\n",
       " ('I', 1873234887),\n",
       " ('are', 1826889845),\n",
       " ('or', 1805149769),\n",
       " ('his', 1672173489),\n",
       " ('from', 1651527506),\n",
       " ('at', 1562321315),\n",
       " ('which', 1557215562),\n",
       " ('he', 1529423270),\n",
       " ('this', 1423199366),\n",
       " ('have', 1372997478),\n",
       " ('had', 1298386780),\n",
       " ('an', 1266190915)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words.most_common(30)  # Just a peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55623fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"Generate a counter with all length n substrings of strings in the input counter\"\"\"\n",
    "    counts = collections.Counter()\n",
    "    for s, count in data.items():\n",
    "        for i in range(len(s) - n + 1):\n",
    "            counts[s[i:i + n]] += count\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b742b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_up_to(data, max_n):\n",
    "    \"\"\"Generate a list of counters with all length 1-maxn substrings of strings in the input counter\"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for s, count in tqdm(data.items(), desc=f\"Collecting (1..{max_n})-grams\"):\n",
    "        for j in range(1, min(len(s), max_n) + 1):\n",
    "            for i in range(len(s) - j + 1):\n",
    "                ret[s[i:i + j]] += count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "111ef2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49146fa18784c2b9fba5c9250649310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting (1..4)-grams:   0%|          | 0/6857277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_words_ngrams = ngrams_up_to(english_words, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e068",
   "metadata": {},
   "source": [
    "## Dictionary extras\n",
    "Padding the dictionary with ngrams by stuff that we expect to appear in the inputs, but which is not contained in the english words dataset.\n",
    "\n",
    "The weights of different symbols are manually estimated and may be significantly non-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518b387",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ec76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numbers_ngrams(p, max_value, max_ngram_length):\n",
    "    ret = collections.Counter()\n",
    "    for i in range(max_value):\n",
    "        weight = p * (1 - p)**i\n",
    "        s = str(i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    for i in range(int(math.log2(max_value))):\n",
    "        weight =  p * (1 - p)**i\n",
    "        s = str(2**i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    for i in range(int(math.log10(max_value))):\n",
    "        weight = 0.5 * p * (1 - p)**i\n",
    "        s = str(10**i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef6e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_ngrams = generate_numbers_ngrams(p=0.25, max_value=10000, max_ngram_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55baca3",
   "metadata": {},
   "source": [
    "### Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8434e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_ngrams = collections.Counter({\n",
    "    \"_\": 1, \" \": .5, \"-\": .2, \"/\": .1, \".\": .1,\n",
    "    \",\": .02, \", \": .2, \". \": .2, \",\\n\": .05, \";\\n\": .1, \"; \": .05,\n",
    "    \"\\r\\n\": .05,\n",
    "    \"http\": .05, \"://\": 0.1, \".com\": .07, \".co\": .005, \"&\": 0.05, \"?\": 0.025,\n",
    "    \"@\": .05\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cecdd2",
   "metadata": {},
   "source": [
    "### Capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc518de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ngrams(ngrams, func):\n",
    "    \"\"\" Map all keys of ngrams \"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for ngram, count in tqdm(ngrams.items()):\n",
    "        ret[func(ngram)] += count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba549d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e911d0d72c24a66981ae60f5cbec13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/832900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caps_english_words_ngrams = map_ngrams(english_words_ngrams, lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5e0c1",
   "metadata": {},
   "source": [
    "## Merging ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ngrams(*weighted_ngrams):\n",
    "    \"\"\" Merge tuples of (weight, ngrams) to a single ngram block.\n",
    "    Weights within each ngram block are normalized first, so that they sum up to 1. \"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for weight, ngrams in tqdm(weighted_ngrams):\n",
    "        factor = weight / sum(ngrams.values())\n",
    "\n",
    "        for ngram, count in tqdm(ngrams.items()):\n",
    "            ret[ngram] += factor * count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2225e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a7d1740c284b7ab214314225cb53ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3379e6f4bde4b26b03b5f23ce48cae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/832900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f341da70fe2943b89b86d0cab270ed02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76724d226ba41ba984e4f466afea1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7871bbf773841febb83b210b4054aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_ngrams = merge_ngrams(\n",
    "    (1, english_words_ngrams),\n",
    "    (0.02, caps_english_words_ngrams),\n",
    "    (0.02, number_ngrams),\n",
    "    (0.05, symbols_ngrams)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e1f8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ngrams(input_ngrams, n):\n",
    "    return collections.Counter({\n",
    "        ngram: count\n",
    "        for (ngram, count)\n",
    "        in heapq.nlargest(\n",
    "            n,\n",
    "            input_ngrams.items(),\n",
    "            key=lambda item: len(item[0]) * item[1]\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6634796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefixes(filtered_ngrams, all_ngrams, nonrepreseneted_weight_fraction=0):\n",
    "    \"\"\"Extends the filtered trie to contain all prefixes. If a prefix is not represeneted in the\n",
    "    input ngrams, its weight is calculated by multiplying the parent ngram weight by `nonrepreseneted_weight_fraction`.\n",
    "    This is mainly useful to artificially  boost their zero probabilities to a reasonable value and avoid very\n",
    "    long huffman codes (which would later be problematic during encoding).\"\"\"\n",
    "    ret = filtered_ngrams.copy()\n",
    "    for ngram in filtered_ngrams:\n",
    "        for i in range(len(ngram)):\n",
    "            substr = ngram[:i+1]\n",
    "            if substr not in ret:\n",
    "                if substr not in all_ngrams:\n",
    "                    ret[substr] = nonrepreseneted_weight_fraction * all_ngrams[ngram]\n",
    "                else:\n",
    "                    ret[substr] = all_ngrams[substr]\n",
    "                \n",
    "    print(f\"Added {len(ret) - len(filtered_ngrams)} ngrams\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a102693",
   "metadata": {},
   "source": [
    "At this point we need to select which ngrams from the merged ones will be used for the final encoding.\n",
    "This is done in a very simple way (function `filter_ngrams`), then the ngrams are padded using `add_prefixes` so that the final trie has an encoding for all inner nodes, which significantly simplifies the encoding.\n",
    "\n",
    "The parameters for these two functions were selected manually by trial and error.\n",
    "The conditions are:\n",
    "- Number of final ngrams is greater or equal than the spread between first items in the trie (not explicit at this point)\n",
    "  - This is done to allow losslessly overlaping the lookup of the first step of the trie by direct lookup with flattened storage of the rest of the trie (details later)\n",
    "- Longest encoded value is shorter than 16 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b261c42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 9 ngrams\n"
     ]
    }
   ],
   "source": [
    "filtered_merged_ngrams = add_prefixes(\n",
    "    filter_ngrams(merged_ngrams, n=82),\n",
    "    merged_ngrams,\n",
    "    nonrepreseneted_weight_fraction=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f98fc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(iterable):\n",
    "    \"\"\"Calculate minimum and maximum of an iterable in a single pass\"\"\"\n",
    "    it = iter(iterable)\n",
    "    try:\n",
    "        minimum = next(it)\n",
    "    except StopIteration:\n",
    "        return (None, None)\n",
    "    maximum = minimum\n",
    "    \n",
    "    for v in it:\n",
    "        if v < minimum:\n",
    "            minimum = v\n",
    "        if v > maximum:\n",
    "            maximum = v\n",
    "            \n",
    "    return (minimum, maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53a561",
   "metadata": {},
   "source": [
    "## Variable length Huffman coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c606a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_tree(symbols):\n",
    "    \"\"\"Build a huffman tree from a dict of symbol: probability. \"\"\"\n",
    "    \n",
    "    heap = [(x[1], True, x[0]) for x in symbols.items()]\n",
    "    # Heap elements look like: probability, leaf flag, (leaf data | children).\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        first = heapq.heappop(heap)\n",
    "        second = heap[0]\n",
    "        \n",
    "        if first > second:\n",
    "            (first, second) = (second, first)\n",
    "        \n",
    "        new = (first[0] + second[0], False, (first, second))\n",
    "        \n",
    "        heapq.heapreplace(heap, new)\n",
    "        \n",
    "    encoding = {}\n",
    "        \n",
    "    def flatten_tree(element, current_encoding=\"\"):\n",
    "        if element[1]:  # is leaf\n",
    "            encoding[element[2]] = current_encoding\n",
    "        else:\n",
    "            flatten_tree(element[2][0], current_encoding + \"0\")\n",
    "            flatten_tree(element[2][1], current_encoding + \"1\")\n",
    "    \n",
    "    flatten_tree(heap[0])\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f26b419b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e' -> 1100 (4b)\n",
      "'t' -> 0111 (4b)\n",
      "'a' -> 0100 (4b)\n",
      "'o' -> 0010 (4b)\n",
      "'i' -> 0000 (4b)\n",
      "'n' -> 11111 (5b)\n",
      "'s' -> 11100 (5b)\n",
      "'r' -> 11011 (5b)\n",
      "'h' -> 10100 (5b)\n",
      "'_' -> 10011 (5b)\n",
      "'l' -> 01011 (5b)\n",
      "'d' -> 00011 (5b)\n",
      "'c' -> 111010 (6b)\n",
      "'u' -> 101101 (6b)\n",
      "'th' -> 101010 (6b)\n",
      "' ' -> 100101 (6b)\n",
      "'m' -> 100100 (6b)\n",
      "'he' -> 100010 (6b)\n",
      "'f' -> 100000 (6b)\n",
      "'p' -> 010100 (6b)\n",
      "'in' -> 001100 (6b)\n",
      "'g' -> 000100 (6b)\n",
      "'the' -> 1111010 (7b)\n",
      "'y' -> 1110111 (7b)\n",
      "'er' -> 1110110 (7b)\n",
      "'w' -> 1101010 (7b)\n",
      "'an' -> 1101001 (7b)\n",
      "'re' -> 1011110 (7b)\n",
      "'on' -> 1011100 (7b)\n",
      "'b' -> 1011001 (7b)\n",
      "'1' -> 1010110 (7b)\n",
      "'at' -> 1000110 (7b)\n",
      "'en' -> 0110111 (7b)\n",
      "'nd' -> 0110011 (7b)\n",
      "'es' -> 0110010 (7b)\n",
      "'ti' -> 0110001 (7b)\n",
      "'v' -> 0101011 (7b)\n",
      "'or' -> 0101010 (7b)\n",
      "', ' -> 0011101 (7b)\n",
      "'-' -> 0011110 (7b)\n",
      "'. ' -> 0011111 (7b)\n",
      "'te' -> 0011010 (7b)\n",
      "'ed' -> 0001010 (7b)\n",
      "'of' -> 11110111 (8b)\n",
      "'is' -> 11110110 (8b)\n",
      "'it' -> 11110011 (8b)\n",
      "'al' -> 11110010 (8b)\n",
      "'ar' -> 11110001 (8b)\n",
      "'nt' -> 11110000 (8b)\n",
      "'to' -> 11010111 (8b)\n",
      "'st' -> 11010110 (8b)\n",
      "'ng' -> 11010000 (8b)\n",
      "'and' -> 10111111 (8b)\n",
      "'ha' -> 10111110 (8b)\n",
      "'se' -> 10111011 (8b)\n",
      "'ou' -> 10111010 (8b)\n",
      "'as' -> 10110001 (8b)\n",
      "'io' -> 10110000 (8b)\n",
      "'le' -> 10101111 (8b)\n",
      "'ve' -> 10101110 (8b)\n",
      "'me' -> 10001111 (8b)\n",
      "'ing' -> 10001110 (8b)\n",
      "'hi' -> 10000111 (8b)\n",
      "'de' -> 10000110 (8b)\n",
      "'ri' -> 10000101 (8b)\n",
      "'ion' -> 01101101 (8b)\n",
      "'ro' -> 01101100 (8b)\n",
      "'ic' -> 01101011 (8b)\n",
      "'co' -> 01101010 (8b)\n",
      "'ra' -> 01101001 (8b)\n",
      "'ea' -> 01101000 (8b)\n",
      "'ne' -> 01100001 (8b)\n",
      "'ce' -> 01100000 (8b)\n",
      "'.' -> 00110111 (8b)\n",
      "'://' -> 00111000 (8b)\n",
      "';\\n' -> 00111001 (8b)\n",
      "'tio' -> 00010111 (8b)\n",
      "'tion' -> 00010110 (8b)\n",
      "'ent' -> 110100011 (9b)\n",
      "'.com' -> 110100010 (9b)\n",
      "'ati' -> 100001001 (9b)\n",
      "'atio' -> 100001000 (9b)\n",
      "'http' -> 001101101 (9b)\n",
      "'ht' -> 0011011000 (10b)\n",
      "',' -> 00110110011 (11b)\n",
      "'.co' -> 001101100101 (12b)\n",
      "':' -> 00110110010001 (14b)\n",
      "':/' -> 00110110010010 (14b)\n",
      "';' -> 00110110010011 (14b)\n",
      "'.c' -> 001101100100001 (15b)\n",
      "'htt' -> 001101100100000 (15b)\n"
     ]
    }
   ],
   "source": [
    "encoding = huffman_tree(filtered_merged_ngrams)\n",
    "for tup, enc in sorted(encoding.items(), key=lambda x: (filtered_merged_ngrams[x[0]]), reverse=True):\n",
    "    print(f\"{tup!r} -> {enc} ({len(enc)}b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee73153",
   "metadata": {},
   "source": [
    "Make sure that the encoding is actually a prefix code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3c9414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup1, enc1 in encoding.items():\n",
    "    for tup2, enc2 in encoding.items():\n",
    "        if tup1 == tup2:\n",
    "            continue\n",
    "        if enc1.startswith(enc2):\n",
    "            print(f\"{b(tup1)} -> {enc1} starts with {b(tup2)} -> {enc2}\")\n",
    "        assert not enc1.startswith(enc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07e3f",
   "metadata": {},
   "source": [
    "### Encoding using a trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af7b53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(encoding):\n",
    "    trie = {}\n",
    "    for tup, enc in sorted(encoding.items(), key=lambda x: x[0]):\n",
    "        current = trie\n",
    "        for c in tup[:-1]:\n",
    "            current = current.setdefault(c, (None, {}))[1]\n",
    "        if tup[-1] in current:\n",
    "            assert current[tup[-1]][0] is None\n",
    "            current[tup[-1]] = (enc, current[tup[-1]][1])\n",
    "        else:\n",
    "            current[tup[-1]] = (enc, {})\n",
    "    \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "457d9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trie(trie, indent=\"\"):\n",
    "    for c, (enc, children) in trie.items():\n",
    "        print(f\"{indent}{c!r}: {enc}\")\n",
    "        print_trie(children, indent + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52c55105",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' ': 100101\n",
      "',': 00110110011\n",
      "  ' ': 0011101\n",
      "'-': 0011110\n",
      "'.': 00110111\n",
      "  ' ': 0011111\n",
      "  'c': 001101100100001\n",
      "    'o': 001101100101\n",
      "      'm': 110100010\n",
      "'1': 1010110\n",
      "':': 00110110010001\n",
      "  '/': 00110110010010\n",
      "    '/': 00111000\n",
      "';': 00110110010011\n",
      "  '\\n': 00111001\n",
      "'_': 10011\n",
      "'a': 0100\n",
      "  'l': 11110010\n",
      "  'n': 1101001\n",
      "    'd': 10111111\n",
      "  'r': 11110001\n",
      "  's': 10110001\n",
      "  't': 1000110\n",
      "    'i': 100001001\n",
      "      'o': 100001000\n",
      "'b': 1011001\n",
      "'c': 111010\n",
      "  'e': 01100000\n",
      "  'o': 01101010\n",
      "'d': 00011\n",
      "  'e': 10000110\n",
      "'e': 1100\n",
      "  'a': 01101000\n",
      "  'd': 0001010\n",
      "  'n': 0110111\n",
      "    't': 110100011\n",
      "  'r': 1110110\n",
      "  's': 0110010\n",
      "'f': 100000\n",
      "'g': 000100\n",
      "'h': 10100\n",
      "  'a': 10111110\n",
      "  'e': 100010\n",
      "  'i': 10000111\n",
      "  't': 0011011000\n",
      "    't': 001101100100000\n",
      "      'p': 001101101\n",
      "'i': 0000\n",
      "  'c': 01101011\n",
      "  'n': 001100\n",
      "    'g': 10001110\n",
      "  'o': 10110000\n",
      "    'n': 01101101\n",
      "  's': 11110110\n",
      "  't': 11110011\n",
      "'l': 01011\n",
      "  'e': 10101111\n",
      "'m': 100100\n",
      "  'e': 10001111\n",
      "'n': 11111\n",
      "  'd': 0110011\n",
      "  'e': 01100001\n",
      "  'g': 11010000\n",
      "  't': 11110000\n",
      "'o': 0010\n",
      "  'f': 11110111\n",
      "  'n': 1011100\n",
      "  'r': 0101010\n",
      "  'u': 10111010\n",
      "'p': 010100\n",
      "'r': 11011\n",
      "  'a': 01101001\n",
      "  'e': 1011110\n",
      "  'i': 10000101\n",
      "  'o': 01101100\n",
      "'s': 11100\n",
      "  'e': 10111011\n",
      "  't': 11010110\n",
      "'t': 0111\n",
      "  'e': 0011010\n",
      "  'h': 101010\n",
      "    'e': 1111010\n",
      "  'i': 0110001\n",
      "    'o': 00010111\n",
      "      'n': 00010110\n",
      "  'o': 11010111\n",
      "'u': 101101\n",
      "'v': 0101011\n",
      "  'e': 10101110\n",
      "'w': 1101010\n",
      "'y': 1110111\n"
     ]
    }
   ],
   "source": [
    "trie = build_trie(encoding)\n",
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8200f",
   "metadata": {},
   "source": [
    "## Trie representation in C++\n",
    "We flatten the trie into an array, using records containing character for this node, number of children, index of a first child and the resulting encoding for this string.\n",
    "First level of the trie is accessed using direct array access with offset, the remaining data are interleaved in the gaps remaining behind this representation.\n",
    "The representation makes sure that the first level access is unique -- character stored in the array corresponds to its location if and only if the item is a first level node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c62c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_trie(trie):\n",
    "    @dataclasses.dataclass\n",
    "    class Record:\n",
    "        c: str\n",
    "        child_index:int\n",
    "        child_count: int\n",
    "        encoding: str\n",
    "        full_key: str  # Debug only\n",
    "    \n",
    "    first_level_bounds = minmax(ord(c) for c in trie.keys())\n",
    "    \n",
    "    placement = {}  # subtree key, index\n",
    "    records = {}\n",
    "    gaps = []\n",
    "    \n",
    "    # Place the fixed first level items, find the usable gaps\n",
    "    previous = 0\n",
    "    for c in sorted(trie.keys()):\n",
    "        index = ord(c) - first_level_bounds[0]\n",
    "        placement[c] = index\n",
    "        gap = index - previous\n",
    "        previous = index + 1\n",
    "        if gap > 0:\n",
    "            gaps.append((gap, index - gap))\n",
    "    gaps.append((float(\"inf\"), first_level_bounds[1] - first_level_bounds[0] + 1)) # There's an infinite gap available at the end\n",
    "    \n",
    "    def assert_gaps():\n",
    "        nonlocal gaps\n",
    "        assert \\\n",
    "            all(g1[1] + g1[0] < g2[1] for g1, g2 in zip(gaps[:-1], gaps[1:])), \\\n",
    "            \"The gaps are ordered and there is at least one non-gap element between them\"\n",
    "    assert_gaps()\n",
    "    \n",
    "    \n",
    "    # Collect the subtrees that need to fill the gaps\n",
    "    def collect_subtrees(trie, root_key, encoding, subtrees):\n",
    "        for c, (child_encoding, child_trie) in trie.items():\n",
    "            collect_subtrees(child_trie, root_key + c, child_encoding, subtrees)\n",
    "        if root_key: # Skip the top level subtree, because we already handle that one differently\n",
    "            subtrees.append((len(trie), root_key, encoding, trie.keys()))\n",
    "    subtrees = []\n",
    "    collect_subtrees(trie, \"\", None, subtrees)\n",
    "    subtrees.sort(reverse=True)\n",
    "    \n",
    "    # Place subtrees into the gaps\n",
    "    for length, key, encoding, children in subtrees:\n",
    "        assert length == len(children)\n",
    "        \n",
    "        possible_gap_indices = sorted((i for i in range(len(gaps)) if gaps[i][0] >= length), key=lambda i: gaps[i][0])\n",
    "        for selected_gap_index in possible_gap_indices:\n",
    "            gap_length, gap_start = gaps[selected_gap_index]\n",
    "            \n",
    "            if any(ord(child) == first_level_bounds[0] + gap_start + i for i, child in enumerate(children)):\n",
    "                print(\"Conflict with top level placement, using different gap\")\n",
    "                continue\n",
    "\n",
    "            #print(f\"placing subtree of key {key!r} (l={length}), into gap {selected_gap_index} ({gap_length}, {gap_start})\")\n",
    "\n",
    "            records[key] = Record(\n",
    "                key[-1],\n",
    "                child_index=gap_start if length > 0 else 255,  # Cosmetic value to make the invalid indices more visible\n",
    "                child_count=length,\n",
    "                encoding=encoding,\n",
    "                full_key=key\n",
    "            )\n",
    "            \n",
    "            # Place all subtree children\n",
    "            for i, child in enumerate(children):\n",
    "                assert (key + child) not in placement\n",
    "                #print(f\"placing {key + child!r} to {i + gap_start}\")\n",
    "                placement[key + child] = gap_start + i\n",
    "\n",
    "            # Shorten or delete the gap\n",
    "            if gap_length == length:\n",
    "                del gaps[selected_gap_index]\n",
    "            else:\n",
    "                gaps[selected_gap_index] = (gap_length - length, gap_start + length)\n",
    "            assert_gaps()\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            raise Exception(\"Can't pack the trie using the greedy algorithm :-(\")\n",
    "    \n",
    "    assert len(gaps) == 1, \"Only one gap left\"\n",
    "    assert gaps[0][0] == float(\"inf\"), \"The remaining gap is the infinite one at the end\"\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    for key, position in sorted(placement.items(), key=lambda x: x[1]):\n",
    "        assert position == len(ret)\n",
    "        ret.append(records[key])\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e13f362",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "packed_trie = pack_trie(trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbf276ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_packed_trie(packed_trie):\n",
    "    def escape_char(c):\n",
    "        assert len(c) == 1\n",
    "        if (32 <= ord(c) < 127) and c not in ('\\\\', '\"'):\n",
    "            return c\n",
    "        else:\n",
    "            return f\"\\\\x{ord(c):02x}\"\n",
    "        \n",
    "    first_level_min = packed_trie[0].c\n",
    "    first_level_max = max(r.c for r in packed_trie if len(r.full_key) == 1)\n",
    "        \n",
    "    print(f\"static constexpr char trieFirstLevelMin = '{escape_char(first_level_min)}';\")\n",
    "    print(f\"static constexpr char trieFirstLevelMax = '{escape_char(first_level_max)}';\")\n",
    "    print(\"static constexpr TrieNode trie[] = {\");\n",
    "    for i, r in enumerate(packed_trie):\n",
    "        c = escape_char(r.c)\n",
    "        end = \"},\" if i < (len(packed_trie) - 1) else \"}\"\n",
    "        print(f\"    {{'{c}', {r.child_index}, {r.child_count}, {len(r.encoding)}, {int(r.encoding, 2)}{end} // {r.full_key!r}\")\n",
    "    print(\"};\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "812fc0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static constexpr char trieFirstLevelMin = ' ';\n",
      "static constexpr char trieFirstLevelMax = 'y';\n",
      "static constexpr TrieNode trie[] = {\n",
      "    {' ', 255, 0, 6, 37}, // ' '\n",
      "    {'a', 255, 0, 8, 104}, // 'ea'\n",
      "    {'d', 255, 0, 7, 10}, // 'ed'\n",
      "    {'n', 53, 1, 7, 55}, // 'en'\n",
      "    {'r', 255, 0, 7, 118}, // 'er'\n",
      "    {'s', 255, 0, 7, 50}, // 'es'\n",
      "    {'l', 255, 0, 8, 242}, // 'al'\n",
      "    {'n', 57, 1, 7, 105}, // 'an'\n",
      "    {'r', 255, 0, 8, 241}, // 'ar'\n",
      "    {'s', 255, 0, 8, 177}, // 'as'\n",
      "    {'t', 56, 1, 7, 70}, // 'at'\n",
      "    {'e', 255, 0, 8, 174}, // 've'\n",
      "    {',', 90, 1, 11, 435}, // ','\n",
      "    {'-', 255, 0, 7, 30}, // '-'\n",
      "    {'.', 23, 2, 8, 55}, // '.'\n",
      "    {'e', 255, 0, 8, 187}, // 'se'\n",
      "    {'t', 255, 0, 8, 214}, // 'st'\n",
      "    {'1', 255, 0, 7, 86}, // '1'\n",
      "    {'c', 255, 0, 8, 107}, // 'ic'\n",
      "    {'n', 50, 1, 6, 12}, // 'in'\n",
      "    {'o', 49, 1, 8, 176}, // 'io'\n",
      "    {'s', 255, 0, 8, 246}, // 'is'\n",
      "    {'t', 255, 0, 8, 243}, // 'it'\n",
      "    {' ', 255, 0, 7, 31}, // '. '\n",
      "    {'c', 62, 1, 15, 6945}, // '.c'\n",
      "    {'n', 255, 0, 8, 22}, // 'tion'\n",
      "    {':', 60, 1, 14, 3473}, // ':'\n",
      "    {';', 58, 1, 14, 3475}, // ';'\n",
      "    {'e', 255, 0, 7, 26}, // 'te'\n",
      "    {'h', 81, 1, 6, 42}, // 'th'\n",
      "    {'i', 64, 1, 7, 49}, // 'ti'\n",
      "    {'o', 255, 0, 8, 215}, // 'to'\n",
      "    {'a', 255, 0, 8, 105}, // 'ra'\n",
      "    {'e', 255, 0, 7, 94}, // 're'\n",
      "    {'i', 255, 0, 8, 133}, // 'ri'\n",
      "    {'o', 255, 0, 8, 108}, // 'ro'\n",
      "    {'f', 255, 0, 8, 247}, // 'of'\n",
      "    {'n', 255, 0, 7, 92}, // 'on'\n",
      "    {'r', 255, 0, 7, 42}, // 'or'\n",
      "    {'u', 255, 0, 8, 186}, // 'ou'\n",
      "    {'d', 255, 0, 7, 51}, // 'nd'\n",
      "    {'e', 255, 0, 8, 97}, // 'ne'\n",
      "    {'g', 255, 0, 8, 208}, // 'ng'\n",
      "    {'t', 255, 0, 8, 240}, // 'nt'\n",
      "    {'a', 255, 0, 8, 190}, // 'ha'\n",
      "    {'e', 255, 0, 6, 34}, // 'he'\n",
      "    {'i', 255, 0, 8, 135}, // 'hi'\n",
      "    {'t', 52, 1, 10, 216}, // 'ht'\n",
      "    {'e', 255, 0, 8, 175}, // 'le'\n",
      "    {'n', 255, 0, 8, 109}, // 'ion'\n",
      "    {'g', 255, 0, 8, 142}, // 'ing'\n",
      "    {'p', 255, 0, 9, 109}, // 'http'\n",
      "    {'t', 51, 1, 15, 6944}, // 'htt'\n",
      "    {'t', 255, 0, 9, 419}, // 'ent'\n",
      "    {'e', 255, 0, 8, 134}, // 'de'\n",
      "    {'o', 255, 0, 9, 264}, // 'atio'\n",
      "    {'i', 55, 1, 9, 265}, // 'ati'\n",
      "    {'d', 255, 0, 8, 191}, // 'and'\n",
      "    {'\\x0a', 255, 0, 8, 57}, // ';\\n'\n",
      "    {'/', 255, 0, 8, 56}, // '://'\n",
      "    {'/', 59, 1, 14, 3474}, // ':/'\n",
      "    {'m', 255, 0, 9, 418}, // '.com'\n",
      "    {'o', 61, 1, 12, 869}, // '.co'\n",
      "    {'_', 255, 0, 5, 19}, // '_'\n",
      "    {'o', 25, 1, 8, 23}, // 'tio'\n",
      "    {'a', 6, 5, 4, 4}, // 'a'\n",
      "    {'b', 255, 0, 7, 89}, // 'b'\n",
      "    {'c', 74, 2, 6, 58}, // 'c'\n",
      "    {'d', 54, 1, 5, 3}, // 'd'\n",
      "    {'e', 1, 5, 4, 12}, // 'e'\n",
      "    {'f', 255, 0, 6, 32}, // 'f'\n",
      "    {'g', 255, 0, 6, 4}, // 'g'\n",
      "    {'h', 44, 4, 5, 20}, // 'h'\n",
      "    {'i', 18, 5, 4, 0}, // 'i'\n",
      "    {'e', 255, 0, 8, 96}, // 'ce'\n",
      "    {'o', 255, 0, 8, 106}, // 'co'\n",
      "    {'l', 48, 1, 5, 11}, // 'l'\n",
      "    {'m', 88, 1, 6, 36}, // 'm'\n",
      "    {'n', 40, 4, 5, 31}, // 'n'\n",
      "    {'o', 36, 4, 4, 2}, // 'o'\n",
      "    {'p', 255, 0, 6, 20}, // 'p'\n",
      "    {'e', 255, 0, 7, 122}, // 'the'\n",
      "    {'r', 32, 4, 5, 27}, // 'r'\n",
      "    {'s', 15, 2, 5, 28}, // 's'\n",
      "    {'t', 28, 4, 4, 7}, // 't'\n",
      "    {'u', 255, 0, 6, 45}, // 'u'\n",
      "    {'v', 11, 1, 7, 43}, // 'v'\n",
      "    {'w', 255, 0, 7, 106}, // 'w'\n",
      "    {'e', 255, 0, 8, 143}, // 'me'\n",
      "    {'y', 255, 0, 7, 119}, // 'y'\n",
      "    {' ', 255, 0, 7, 29} // ', '\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "format_packed_trie(packed_trie)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
