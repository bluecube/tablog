{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-softball",
   "metadata": {},
   "source": [
    "# String predictor\n",
    "\n",
    "Tablog encodes column name strings in headers and we want to compress them at least a bit -- the strings should not be the dominant part of the dataset, but still saving few bytes is always good.\n",
    "\n",
    "This file contains experiments that should help determining an appropriate compression algorithm.\n",
    "\n",
    "Goals for this part are (in order of importance):\n",
    "1. fast and light\n",
    "2. simple-ishto implement\n",
    "3. compresses well enough to make me happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86d5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "import heapq\n",
    "import itertools\n",
    "import string\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d5d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17054480",
   "metadata": {},
   "source": [
    "## Manual test data\n",
    "These are just a couple of strings that I feel will be representative of the intended use of Tablog.\n",
    "Used mostly to get an intuitive feel of how the compression behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test_data = [\n",
    "    \"deadbeef\", \"hello_world\", \"voltage\", \"current\", \"left\", \"column58\", \"left_motor_speed\", \"kP\", \"right_motor_kD\",\n",
    "    \"POWER!\", \"timestamp\", \"AverageTicks\", \"temperature\", \"MCU-power\", \"alpha\", \"beta\", \"omega\", \"velocityX\",\n",
    "    \"!@#$%^&*()\", \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-parking",
   "metadata": {},
   "source": [
    "## Harvesting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc6b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_from_example_directories(regex):\n",
    "    \"\"\"Recursively go through all source-looking files in the\n",
    "    directories and return a counter of words matching regex.\"\"\"\n",
    "    \n",
    "    extensions = (\".cpp\", \".hpp\", \".c\", \".h\", \".py\", \".sh\")\n",
    "    directories = [\n",
    "        \"/home/cube/development/c/tablog/encoder\",\n",
    "        \"/home/cube/development/c/tablog/decoder\",\n",
    "        \"/home/cube/development/c/tablog/integration_tests\",\n",
    "        \"/usr/src/linux\"\n",
    "    ]\n",
    "\n",
    "    regex = re.compile(regex)\n",
    "    \n",
    "    ret = collections.Counter()\n",
    "    \n",
    "    for directory in tqdm(directories, desc=\"Looking through directories\"):\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if not file.endswith(extensions):\n",
    "                    continue\n",
    "                path = os.path.join(root, file)\n",
    "                with open(path, \"r\") as fp:\n",
    "                    for line in fp:\n",
    "                        for match in regex.finditer(line):\n",
    "                            ret[match.group(0)] += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-academy",
   "metadata": {},
   "source": [
    "Load strings that look like identifiers from Tablog sources and  from Linux kernel sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0c246a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = regex_from_example_directories(r\"(?<![a-zA-Z0-9_])[a-zA-Z_][a-zA-Z0-9_]*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba44f6",
   "metadata": {},
   "source": [
    "## English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b27a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_books_1grams_words():\n",
    "    ret = collections.Counter()\n",
    "    good_string_re = re.compile(r\"^([a-zA-Z]*)\\t[^\\t]*\\t([0-9]*)\")\n",
    "    for letter in tqdm(string.ascii_lowercase):\n",
    "        r = requests.get(f\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-{letter}.gz\", stream=True)\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=file_size, desc=letter) as compressed_response_file:\n",
    "            with gzip.open(compressed_response_file, \"rt\") as response_file:\n",
    "                for line in response_file:\n",
    "                    if match := good_string_re.match(line):\n",
    "                        ret[match[1]] += int(match[2])\n",
    "                        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf35224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b6a46db8ce45e68a01bc0738017311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee4c1559f83497aa2c3b11c1b9eb10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a:   0%|          | 0/343487895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4383c4c64f6146dca960d9b347f6c2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b:   0%|          | 0/247964820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68484fcf1f8a474fb02eab0afc96c3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c:   0%|          | 0/393712861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d7a8ca522c464b90116b6cdaa08b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "d:   0%|          | 0/239389478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948e31be00df4c3db45e6e28dffafce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "e:   0%|          | 0/204996476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d78b344e264e21965c24caa10826d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "f:   0%|          | 0/185305580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6133a17cd54ad38769b1d0e9e4187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "g:   0%|          | 0/160767731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3117ab987188477ab10e673a7c46861f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "h:   0%|          | 0/183438682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a44fec6bae4f70aeae464cb91392b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "i:   0%|          | 0/203553215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6095ed506544c1a8f74b2eebf6e2fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/64839808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a90c8d1834545d6905262b7327fb007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "k:   0%|          | 0/109307817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a994f1c1ca0243fa8c9b623580b2c6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "l:   0%|          | 0/188008415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfba4f980144994877ddc2d699f4df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "m:   0%|          | 0/289750659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6512792b1345959c15847013ac99b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n:   0%|          | 0/141931087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58003e6fe18a48429cb20bf315d09efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "o:   0%|          | 0/139934943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e86bc179c384f899900535948b1a644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "p:   0%|          | 0/357808228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef15bc7b9ba741f4bd023e8784537fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "q:   0%|          | 0/26608642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dad2e8847d4083955ef5bcc18bf991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r:   0%|          | 0/216873218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a139e75a7488d95abeb43dc0bf427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "s:   0%|          | 0/444315601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae8a022779f4bb2a54dd560d17a94e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "t:   0%|          | 0/265071292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc5aba78251437699456dc876b0488d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "u:   0%|          | 0/88124330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d89c3431574907ad3526e6c85666a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v:   0%|          | 0/108848534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1f4b428a4a4fea93c8c200e068e946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "w:   0%|          | 0/118835349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dabb203cc2b40e78ba917b4a5a81221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "x:   0%|          | 0/14649720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd29a720ec74bf6854caa12384657dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "y:   0%|          | 0/26595711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062df6082ee9413bbdb2f26feecd429d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "z:   0%|          | 0/27324007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_words = google_books_1grams_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8da69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23688414489),\n",
       " ('of', 15342397280),\n",
       " ('and', 11021132912),\n",
       " ('to', 9494905988),\n",
       " ('in', 7611765281),\n",
       " ('a', 7083003595),\n",
       " ('is', 4139526351),\n",
       " ('that', 3870260345),\n",
       " ('for', 3021925527),\n",
       " ('The', 2763139452),\n",
       " ('was', 2737725870),\n",
       " ('as', 2626386982),\n",
       " ('with', 2504225400),\n",
       " ('be', 2392576396),\n",
       " ('by', 2249978492),\n",
       " ('not', 2208497726),\n",
       " ('it', 2204134503),\n",
       " ('on', 2143313303),\n",
       " ('I', 1873234887),\n",
       " ('are', 1826889845),\n",
       " ('or', 1805149769),\n",
       " ('his', 1672173489),\n",
       " ('from', 1651527506),\n",
       " ('at', 1562321315),\n",
       " ('which', 1557215562),\n",
       " ('he', 1529423270),\n",
       " ('this', 1423199366),\n",
       " ('have', 1372997478),\n",
       " ('had', 1298386780),\n",
       " ('an', 1266190915)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87233c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substring_compositions(s):\n",
    "    \"\"\"Yield all sequences of substring that concatenate to s. If len(s) >= 1 then the first item is always [s]\"\"\"\n",
    "    if not len(s):\n",
    "        yield []\n",
    "    for i in range(len(s)):\n",
    "        for substring in all_substring_compositions(s[:i]):\n",
    "            yield substring + [s[i:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55623fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"Generate a counter with all length n substrings of strings in the input counter\"\"\"\n",
    "    counts = collections.Counter()\n",
    "    for s, count in data.items():\n",
    "        for i in range(len(s) - n + 1):\n",
    "            counts[s[i:i + n]] += count\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b742b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_up_to(data, max_n):\n",
    "    \"\"\"Generate a list of counters with all length 1-maxn substrings of strings in the input counter\"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for s, count in tqdm(data.items(), desc=f\"Collecting (1..{max_n})-grams\"):\n",
    "        for j in range(1, max_n + 1):\n",
    "            for i in range(len(s) - j + 1):\n",
    "                ret[s[i:i + j]] += count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "111ef2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de69e29c3ddd4816b58330e35a388f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting (1..5)-grams:   0%|          | 0/6857277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_words_ngrams = ngrams_up_to(english_words, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e068",
   "metadata": {},
   "source": [
    "## Dictionary extras\n",
    "Padding the dictionary with ngrams by stuff that we expect to appear in the inputs, but which is not contained in the english words dataset.\n",
    "\n",
    "The weights of different symbols are manually estimated and may be significantly non-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518b387",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "11ec76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numbers_ngrams(p, max_value, max_ngram_length):\n",
    "    ret = collections.Counter()\n",
    "    for i in range(max_value):\n",
    "        weight = p * (1 - p)**i\n",
    "        s = str(i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    for i in range(int(math.log2(max_value))):\n",
    "        weight =  p * (1 - p)**i\n",
    "        s = str(2**i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    for i in range(int(math.log10(max_value))):\n",
    "        weight = 2 * p * (1 - p)**i\n",
    "        s = str(10**i)\n",
    "        for n in range(1, max_ngram_length + 1):\n",
    "            for k in range(len(s) - n + 1):\n",
    "                ret[s[k:k + n]] += weight\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8ef6e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_ngrams = generate_numbers_ngrams(p=0.25, max_value=10000, max_ngram_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55baca3",
   "metadata": {},
   "source": [
    "### Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ee839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ngrams(*weighted_strings):\n",
    "    \"\"\" Generate a length-separated list with ngrams directly provided. Argument is (weight, ngram) tuples. \"\"\"\n",
    "    ret = []\n",
    "    for weight, ngram in weighted_strings:\n",
    "        while len(ret) <= len(ngram):\n",
    "            ret.append(collections.Counter())\n",
    "        \n",
    "        ret[len(ngram)][ngram] += weight\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8434e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_ngrams = collections.Counter([\n",
    "    (\"_\", 1), (\" \", .5), (\"-\", .2),\n",
    "    (\".\", .2), (.5, \",\"), (\", \", .2), (\". \", .2), (.1, \",\\n\"), (.2, \";\\n\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cecdd2",
   "metadata": {},
   "source": [
    "### Capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cc518de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ngrams(ngrams, func):\n",
    "    \"\"\" Map all keys of ngrams \"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for ngram, count in tqdm(ngrams.items()):\n",
    "        ret[func(ngram)] += count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ba549d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4314cef14a04a54a8e03c659578686c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3266865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caps_english_words_ngrams = map_ngrams(english_words_ngrams, lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5e0c1",
   "metadata": {},
   "source": [
    "## Merging ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e94dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ngrams(*weighted_ngrams):\n",
    "    \"\"\" Merge tuples of (weight, ngrams) to a single ngram block.\n",
    "    Weights within each ngram block are normalized first, so that they sum up to 1. \"\"\"\n",
    "    ret = collections.Counter()\n",
    "    for weight, ngrams in tqdm(weighted_ngrams):\n",
    "        factor = weight / sum(ngrams.values())\n",
    "\n",
    "        for ngram, count in tqdm(ngrams.items()):\n",
    "            ret[ngram] += factor * count\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e862d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ngrams(data):\n",
    "    total_count = 0\n",
    "    for ngrams, total in data:\n",
    "        for ngram, count in ngrams.most_common():\n",
    "            total_count += 1\n",
    "            print(f\"{ngram!r}: {100 * count / total:.1f}%\")\n",
    "    print(f\"Total {total_count} different ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d2225e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1338c5fe3244eca9400ed8fb378b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874d693881424b809923510698ca0c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3266865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe06fadd56f4d7cbec4767f2007b13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1657513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3076da0466654e7e976fa064a9311152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d773bbefd144789f194f16b2e8c3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_ngrams = merge_ngrams(\n",
    "    (1, english_words_ngrams),\n",
    "    (0.02, caps_english_words_ngrams),\n",
    "    (0.02, number_ngrams),\n",
    "    (0.05, symbols_ngrams)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3e1f8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ngrams(input_ngrams, min_goodness, max_n=None):\n",
    "    def ngram_goodness(ngram_length, ngram_probability):\n",
    "        \"\"\"Return an estimate of how much this ngram will help with making the encoding smaller.\"\"\"\n",
    "        # TODO: This is based just on my feeling on how the filtering should behave...\n",
    "        encoded_length = -math.log(ngram_probability)\n",
    "        return ngram_probability * ngram_length / encoded_length\n",
    "    \n",
    "    if max_n is None:\n",
    "        max_n = len(input_ngrams)\n",
    "    else:\n",
    "        max_n = min(len(input_ngrams), max_n)\n",
    "    \n",
    "    filtered_ngrams = [input_ngrams[0].copy()]\n",
    "    totals = [sum(ngrams.values()) for ngrams in input_ngrams]\n",
    "    \n",
    "    for n in trange(2, max_n + 1):\n",
    "        current_ngrams = input_ngrams[n - 1]\n",
    "        current_total = totals[n - 1]\n",
    "        \n",
    "        current_ngrams_to_keep = collections.Counter()\n",
    "        current_ngram_count = len(current_ngrams)\n",
    "                \n",
    "        for ngram, count in tqdm(current_ngrams.items(), desc=f\"Filtering {n}-tuples\"):\n",
    "            ngram_probability = count / current_total\n",
    "            if ngram_goodness(n, ngram_probability) < min_goodness:\n",
    "                continue\n",
    "            composition_probability = 0  # How probable is it that we find replicate this ngram as a combination of shorter ones\n",
    "            for composition in itertools.islice(all_substring_compositions(ngram), 1, None):\n",
    "                this_composition_probability = 1\n",
    "                try:\n",
    "                    for substring in composition:\n",
    "                        substring_probability = filtered_ngrams[len(substring) - 1][substring] / totals[len(substring) - 1]\n",
    "                        this_composition_probability *= substring_probability\n",
    "                except KeyError:  # The substring of current ngram might not be in the list if it was previously pruned\n",
    "                    pass\n",
    "                else:\n",
    "                    composition_probability += this_composition_probability\n",
    "                    if composition_probability > ngram_probability:\n",
    "                        break\n",
    "\n",
    "            if ngram_probability > composition_probability:\n",
    "                current_ngrams_to_keep[ngram] = count\n",
    "\n",
    "        print(f\"Keeping {len(current_ngrams_to_keep)} ({100 * len(current_ngrams_to_keep) / current_ngram_count:.0f}%) {n}-grams\")\n",
    "        filtered_ngrams.append(current_ngrams_to_keep)\n",
    "    \n",
    "        for i, sub_ngrams in enumerate(ngrams_up_to(filtered_ngrams[n - 1], n - 1)):\n",
    "            filtered_ngrams[i] -= sub_ngrams\n",
    "    \n",
    "    for i, (current_ngrams, current_total) in enumerate(zip(filtered_ngrams, totals)):\n",
    "        for ngram, count in current_ngrams.items():\n",
    "            ngram_probability = count / current_total\n",
    "            if ngram_goodness(n, ngram_probability) < min_goodness:\n",
    "                continue\n",
    "    \n",
    "    return [(ngrams, total) for ngrams, total in zip(filtered_ngrams, totals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6b261c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [154]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_merged_ngrams \u001b[38;5;241m=\u001b[39m filter_ngrams(merged_ngrams, \u001b[38;5;241m0.01\u001b[39m, max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Input \u001b[0;32mIn [151]\u001b[0m, in \u001b[0;36mfilter_ngrams\u001b[0;34m(input_ngrams, min_goodness, max_n)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_ngrams), max_n)\n\u001b[0;32m---> 13\u001b[0m filtered_ngrams \u001b[38;5;241m=\u001b[39m [\u001b[43minput_ngrams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()]\n\u001b[1;32m     14\u001b[0m totals \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(ngrams\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mfor\u001b[39;00m ngrams \u001b[38;5;129;01min\u001b[39;00m input_ngrams]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m2\u001b[39m, max_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "filtered_merged_ngrams = filter_ngrams(merged_ngrams, 0.01, max_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07c39254",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e': 6.8%\n",
      "'a': 6.4%\n",
      "'o': 6.2%\n",
      "'i': 6.0%\n",
      "'n': 5.9%\n",
      "'s': 5.1%\n",
      "'r': 5.1%\n",
      "'t': 3.8%\n",
      "'1': 3.5%\n",
      "'l': 3.3%\n",
      "'d': 3.0%\n",
      "'c': 2.6%\n",
      "'0': 2.4%\n",
      "'u': 2.2%\n",
      "'m': 1.9%\n",
      "'f': 1.9%\n",
      "'p': 1.6%\n",
      "'g': 1.5%\n",
      "'y': 1.3%\n",
      "'w': 1.3%\n",
      "'E': 1.2%\n",
      "'b': 1.1%\n",
      "'T': 1.1%\n",
      "'2': 1.0%\n",
      "'A': 0.9%\n",
      "'I': 0.9%\n",
      "'v': 0.8%\n",
      "'S': 0.8%\n",
      "'O': 0.7%\n",
      "'N': 0.7%\n",
      "'R': 0.6%\n",
      "'4': 0.6%\n",
      "'H': 0.5%\n",
      "'6': 0.5%\n",
      "'C': 0.5%\n",
      "'L': 0.4%\n",
      "'k': 0.4%\n",
      "'D': 0.4%\n",
      "'8': 0.4%\n",
      "'3': 0.4%\n",
      "'M': 0.4%\n",
      "'P': 0.3%\n",
      "'F': 0.3%\n",
      "'U': 0.3%\n",
      "'B': 0.3%\n",
      "'G': 0.2%\n",
      "'W': 0.2%\n",
      "'5': 0.2%\n",
      "'x': 0.2%\n",
      "'Y': 0.2%\n",
      "'V': 0.1%\n",
      "'7': 0.1%\n",
      "'q': 0.1%\n",
      "'K': 0.1%\n",
      "'j': 0.1%\n",
      "'z': 0.1%\n",
      "'J': 0.1%\n",
      "'9': 0.1%\n",
      "'X': 0.0%\n",
      "'Q': 0.0%\n",
      "'Z': 0.0%\n",
      "'_': 4.1%\n",
      "' ': 2.1%\n",
      "'.': 2.1%\n",
      "',': 2.1%\n",
      "'<': 2.1%\n",
      "'>': 2.1%\n",
      "'th': 0.7%\n",
      "'he': 0.6%\n",
      "',\\n': 5.5%\n",
      "';\\n': 5.5%\n",
      "'=\"': 2.7%\n",
      "'/>': 2.7%\n",
      "'the': 2.1%\n",
      "Total 74 different ngrams\n"
     ]
    }
   ],
   "source": [
    "print_ngrams(filtered_merged_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-narrative",
   "metadata": {},
   "source": [
    "# Analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(iterable, key=lambda x: x):\n",
    "    it =  iter(iterable)\n",
    "    try:\n",
    "        min_v = next(it)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    max_v = min_v\n",
    "    min_k = key(min_v)\n",
    "    max_k = min_k\n",
    "    \n",
    "    for v in it:\n",
    "        k = key(v)\n",
    "        if k < min_k:\n",
    "            min_v = v\n",
    "            min_k = k\n",
    "        if k > max_k:\n",
    "            max_v = v\n",
    "            max_k = k\n",
    "    \n",
    "    return min_v, max_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53a561",
   "metadata": {},
   "source": [
    "## Variable length Huffman coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c606a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_tree(symbols):\n",
    "    \"\"\"Build a huffman tree from a dict of symbol: probability. \"\"\"\n",
    "    \n",
    "    heap = [(x[1], True, x[0]) for x in symbols.items()]\n",
    "    # Heap elements look like: probability, leaf flag, (leaf data | children).\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        first = heapq.heappop(heap)\n",
    "        second = heap[0]\n",
    "        \n",
    "        if first > second:\n",
    "            (first, second) = (second, first)\n",
    "        \n",
    "        new = (first[0] + second[0], False, (first, second))\n",
    "        \n",
    "        heapq.heapreplace(heap, new)\n",
    "        \n",
    "    encoding = {}\n",
    "        \n",
    "    def flatten_tree(element, current_encoding=\"\"):\n",
    "        if element[1]:  # is leaf\n",
    "            encoding[element[2]] = current_encoding\n",
    "        else:\n",
    "            flatten_tree(element[2][0], current_encoding + \"0\")\n",
    "            flatten_tree(element[2][1], current_encoding + \"1\")\n",
    "    \n",
    "    flatten_tree(heap[0])\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2eee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned_huffman_tree(symbols, max_bits, require_full_trie, max_items=float(\"inf\")):\n",
    "    all_symbols = symbols\n",
    "    symbols = all_symbols.copy()\n",
    "    \n",
    "    while True:\n",
    "        encoding = huffman_tree(symbols)\n",
    "                \n",
    "        max_bits_used = max(len(x) for x in encoding.values())\n",
    "        \n",
    "        required_symbols = set()\n",
    "        if require_full_trie:\n",
    "            for symbol in symbols:\n",
    "                required_symbols |= prefixes(symbol)\n",
    "            \n",
    "        def expected_shortening(x):\n",
    "            \"\"\"Return expected decrease of size of the output if we include this item\"\"\"\n",
    "            return (8 * len(x) - len(encoding[x])) * symbols[x]\n",
    "        \n",
    "        def victim_key(x):\n",
    "            \"\"\"Use probability of the symbol as a secondary key\"\"\"\n",
    "            return (expected_shortening(x), symbols[x]) \n",
    "\n",
    "        worst_sym, best_sym = minmax(encoding.keys(), key=victim_key)\n",
    "        \n",
    "        def format_symbol(x):\n",
    "            return f\"{b(x)} ({len(encoding[x])}b, p = {100 * symbols[x]:.2f}%, CR = {100 * len(encoding[x]) / (8 * len(x)):.2f}%, expected shortening = {expected_shortening(x):.3f}b)\"\n",
    "        \n",
    "        print(f\"Encoded {len(encoding)} symbols, max code length {max_bits_used}, {len(required_symbols)} required symbols\")\n",
    "        print(\"  Best symbol\", format_symbol(best_sym))\n",
    "        print(\"  Worst symbol\", format_symbol(worst_sym))\n",
    "        \n",
    "        victim = min(symbols.keys() - required_symbols, key=victim_key)\n",
    "        \n",
    "        if \\\n",
    "            expected_shortening(victim) > 0 and \\\n",
    "            max_bits_used <= max_bits and \\\n",
    "            len(encoding) <= max_items:\n",
    "            return encoding\n",
    "\n",
    "        print(\"Removing symbol\", format_symbol(victim))\n",
    "        \n",
    "        del symbols[victim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = pruned_huffman_tree(symbols, 12, require_full_trie=False)\n",
    "#encoding = huffman_tree(symbols)\n",
    "print(len(encoding))\n",
    "for tup, enc in sorted(encoding.items(), key=lambda x: (symbols[x[0]]), reverse=True):\n",
    "    print(f\"{b(tup)} -> {enc} ({len(enc)}b, p = {100 * symbols[tup]:.2f}%, expected_shortening = {symbols[tup] * (8 * len(tup) - len(enc)):.3f}b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee73153",
   "metadata": {},
   "source": [
    "Make sure that the encoding is actually a prefix code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup1, enc1 in encoding.items():\n",
    "    for tup2, enc2 in encoding.items():\n",
    "        if tup1 == tup2:\n",
    "            continue\n",
    "        if enc1.startswith(enc2):\n",
    "            print(f\"{b(tup1)} -> {enc1} starts with {b(tup2)} -> {enc2}\")\n",
    "        assert not enc1.startswith(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece29032",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dict = {\"\".join(chr(x) for x in key).encode(\"ascii\"): e for key, e in encoding.items()}\n",
    "encoding_dict_key_length = max(len(key) for key in encoding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07e3f",
   "metadata": {},
   "source": [
    "### Encoding using a trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(encoding):\n",
    "    trie = {}\n",
    "    for tup, enc in sorted(encoding.items(), key=lambda x: x[0]):\n",
    "        current = trie\n",
    "        for c in tup[:-1]:\n",
    "            current = current.setdefault(c, (None, {}))[1]\n",
    "        if tup[-1] in current:\n",
    "            assert current[tup[-1]][0] is None\n",
    "            current[tup[-1]] = (enc, current[tup[-1]][1])\n",
    "        else:\n",
    "            current[tup[-1]] = (enc, {})\n",
    "    \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trie(trie, indent=\"\"):\n",
    "    for c, (enc, children) in trie.items():\n",
    "        print(f\"{indent}{b(c)}: {enc}\")\n",
    "        print_trie(children, indent + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c55105",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trie = build_trie(encoding)\n",
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ff13e",
   "metadata": {},
   "source": [
    "### Encoding using perfect hashing\n",
    "This uses ideas from gperf (https://www.dre.vanderbilt.edu/~schmidt/PDF/gperf.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca49f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_perfect_hash(encoding):\n",
    "    min_c, max_c = minmax(c for tup in encoding.keys() for c in tup)\n",
    "    \n",
    "    print(\"total key characters\", sum(len(key) for key in encoding.keys()))\n",
    "    \n",
    "    key_lengths = collections.Counter(len(key) for key in encoding.keys())\n",
    "    print(key_lengths)\n",
    "    \n",
    "    print(sum(key_lengths.values()))\n",
    "    \n",
    "    asso_size = max_c - min_c + 2  # + extra item for out of range\n",
    "    print(max_c - min_c)\n",
    "    asso = [0] * asso_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_perfect_hash(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "72**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-plaza",
   "metadata": {},
   "source": [
    "## Some predictor ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ab24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_error_size(e):\n",
    "    \"\"\" Return size in bits of given error encoded using tablog's stream encoder, assuming perfect adaptation \"\"\"\n",
    "    if e == 0:\n",
    "         return 1\n",
    "    else:\n",
    "         return 2 + 1 + math.floor(1 + math.log2(abs(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_number_size(n):\n",
    "    return 2 * math.floor(math.log2(n + 1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b87974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictor(method):\n",
    "    print(f\"Evaluating {method.__name__}\")\n",
    "    strings = 0\n",
    "    log_bits_per_char_sum = 0\n",
    "    log_bits_per_char_unique_sum = 0\n",
    "    for s, count in tqdm(data.items()):\n",
    "        string_bits = method(s)\n",
    "        bits_per_char = string_bits / len(s)\n",
    "        \n",
    "        log_bits_per_char_sum += count * math.log(string_bits / len(s))\n",
    "        log_bits_per_char_unique_sum += math.log(string_bits / len(s))\n",
    "        \n",
    "        strings += count\n",
    "\n",
    "    print(f\"    geometric mean {math.exp(log_bits_per_char_sum / strings):.2f} bits/char\")\n",
    "    print(f\"    geometric mean (unique) {math.exp(log_bits_per_char_unique_sum / len(data)):.2f} bits/char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_predictor(s):\n",
    "    ord_e = ord(\"e\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        sum(encoded_error_size(c - ord_e) for c in s)\n",
    "evaluate_predictor(e_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_char_predictor(s):\n",
    "    first_prediction = ord(\"e\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        encoded_error_size(s[0] - first_prediction) + \\\n",
    "        sum(encoded_error_size(c - prev) for c, prev in zip(s[1:], s[:-1]))\n",
    "evaluate_predictor(same_char_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_predictor(s):\n",
    "    two_c = 215\n",
    "    first_prediction = ord(\"s\")\n",
    "    \n",
    "    return \\\n",
    "        encoded_number_size(len(s)) + \\\n",
    "        encoded_error_size(s[0] - first_prediction) + \\\n",
    "        sum(encoded_error_size(c - (two_c - prev)) for c, prev in zip(s[1:], s[:-1]))\n",
    "evaluate_predictor(reflection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_style(s):\n",
    "    return 8 + 8 * len(s)\n",
    "evaluate_predictor(c_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_freq_table(s):\n",
    "    #freq_table = b\"etisarnodlc_puf\"\n",
    "    freq_table = b\"etisarnodlc_pufmbghyvxTwkERICSz\"\n",
    "    \n",
    "    ret = encoded_number_size(len(s))\n",
    "    \n",
    "    for c in s:\n",
    "        try:\n",
    "            index = freq_table.index(c)\n",
    "        except ValueError:\n",
    "            ret += 1 + 8\n",
    "        else:\n",
    "            ret += encoded_number_size(index + 1)\n",
    "            \n",
    "    return ret\n",
    "evaluate_predictor(small_freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_verb_lengths(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    verbatim_length = 0\n",
    "    verbatim_size = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            if verbatim_length > 0:\n",
    "                encoded_length = 1 + encoded_number_size(verbatim_length - 1) + verbatim_size\n",
    "                if printing:\n",
    "                    print(f\"{verbatim_length} verbatim characters: {encoded_length}b, {encoded_length/verbatim_length:.2f}b/char\")\n",
    "                ret += encoded_length\n",
    "                verbatim_length = 0\n",
    "                verbatim_size = 0\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            i += 1\n",
    "            verbatim_length += 1\n",
    "            verbatim_size += 8\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "            \n",
    "    if verbatim_length > 0:\n",
    "        encoded_length = 1 + encoded_number_size(verbatim_length - 1) + verbatim_size\n",
    "        if printing:\n",
    "            print(f\"(flush) {verbatim_length} verbatim characters: {encoded_length}b, {encoded_length/verbatim_length:.2f}b/char\")\n",
    "        ret += 1 + encoded_number_size(verbatim_length) + verbatim_size\n",
    "        verbatim_length = 0\n",
    "        verbatim_size = 0\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_verb_lengths)\n",
    "#Evaluating huffman_dict_verb_lengths\n",
    "#    geometric mean 6.99 bits/char\n",
    "#    geometric mean (unique) 6.57 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20809e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def huffman_dict(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            encoded_length = 1 + 8\n",
    "            if printing:\n",
    "                print(f\"{b(s[i])} verbatim: {encoded_length}b\")\n",
    "            ret += encoded_length\n",
    "            i += 1\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict)\n",
    "#Evaluating huffman_dict\n",
    "#    geometric mean 6.95 bits/char\n",
    "#    geometric mean (unique) 6.52 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a050335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_diff_verb(s, printing=False):\n",
    "    ret = encoded_number_size(len(s))\n",
    "\n",
    "    i = 0\n",
    "    previous = ord('e')\n",
    "    while True:\n",
    "        encoded = None\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            encoded_length = 1 + len(encoded)\n",
    "            if printing:\n",
    "                print(f\"{s[i:i+j]} in dict: {encoded_length}b, {encoded_length/j:.2f}b/char\")\n",
    "            ret += encoded_length\n",
    "            previous = s[i + j - 1]\n",
    "            i += j\n",
    "            break\n",
    "            \n",
    "        if encoded is None:\n",
    "            encoded_length = 1\n",
    "            error = s[i] - previous\n",
    "            if error == 0:\n",
    "                encoded_length += 1\n",
    "            else:\n",
    "                abs_error = abs(error) - 1\n",
    "                fixed_bits = 4\n",
    "                encoded_length += encoded_number_size(abs_error >> fixed_bits) + fixed_bits\n",
    "            if printing:\n",
    "                print(f\"{b(s[i])} not in dict: {encoded_length}b\")\n",
    "            ret += encoded_length\n",
    "            previous = s[i]\n",
    "            i += 1\n",
    "    \n",
    "        if i >= len(s):\n",
    "            break\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{ret / len(s):.2f} bits/char\")\n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_diff_verb)\n",
    "#Evaluating huffman_dict_diff_verb\n",
    "#    geometric mean 6.90 bits/char\n",
    "#    geometric mean (unique) 6.40 bits/char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5473c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_dict_grouped_hits(s, printing=False):\n",
    "    global encoding_dict, encoding_dict_key_length\n",
    "    \n",
    "    def match_at(i):\n",
    "        \"\"\"Return number of characters of the string at position i that match the dictionary and the encoded length\"\"\"\n",
    "        for j in reversed(range(1, 1 + min(encoding_dict_key_length, len(s) - i))):\n",
    "            try:\n",
    "                encoded = encoding_dict[s[i:i+j]]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            else:\n",
    "                return j, len(encoded)\n",
    "        return (0, None)\n",
    "    \n",
    "    def count_matches(i):\n",
    "        \"\"\"Return number of successive dictionary matches/mismatches starting at i and whether we finished at the end of the string\"\"\"\n",
    "        count = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if match_length:\n",
    "                count += 1\n",
    "                i += match_length\n",
    "            else:\n",
    "                return count, False\n",
    "        return count, True\n",
    "    \n",
    "    def count_mismatches(i):\n",
    "        \"\"\"Return number of successive dictionary matches/mismatches starting at i and whether we finished at the end of the string\"\"\"\n",
    "        count = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if not match_length:\n",
    "                count += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                return count, False\n",
    "        return count, True\n",
    "    \n",
    "    def encoded_matches(i):\n",
    "        length = 0\n",
    "        encoded_size = 0\n",
    "        while i < len(s):\n",
    "            match_length, match_encoded_size = match_at(i)\n",
    "            if match_length:\n",
    "                if printing:\n",
    "                    print(f\"  {s[i:i+match_length]}: {match_encoded_size}b, {match_encoded_size/match_length:.2f}b/char\")\n",
    "                i += match_length\n",
    "                length += match_length\n",
    "                encoded_size += match_encoded_size\n",
    "            else:\n",
    "                break\n",
    "        return length, encoded_size\n",
    "    \n",
    "    def encoded_mismatches(i, prev):\n",
    "        length = 0\n",
    "        encoded_size = 0\n",
    "        while i < len(s):\n",
    "            match_length, _ = match_at(i)\n",
    "            if not match_length:\n",
    "                error = s[i] - prev\n",
    "                prev = s[i]\n",
    "                if error == 0:\n",
    "                    e = 1\n",
    "                else:\n",
    "                    abs_error = abs(error) - 1\n",
    "                    fixed_bits = 3\n",
    "                    e = encoded_number_size(abs_error >> fixed_bits) + fixed_bits\n",
    "                \n",
    "                if printing:\n",
    "                    print(f\"  {bytes([s[i]])}: {e}b\")\n",
    "                i += 1\n",
    "                length += 1\n",
    "                encoded_size += e\n",
    "            else:\n",
    "                break\n",
    "        return length, encoded_size\n",
    "    \n",
    "    ret = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Handle a first block separately, we allow zero number of matches in it\n",
    "    block_length, end = count_matches(i)\n",
    "    if printing:\n",
    "        print(f\"First match block: {block_length}\")\n",
    "    ret += encoded_number_size(block_length)\n",
    "        # Might be zero, if:\n",
    "        # a) Starting with a non-match character\n",
    "        # b) Processing an empty string\n",
    "    length, encoded_size = encoded_matches(i)\n",
    "    i += length\n",
    "    ret += encoded_size\n",
    "    \n",
    "    if i > 0:\n",
    "        prev = s[i - 1]\n",
    "    else:\n",
    "        prev = ord(\"e\")\n",
    "    \n",
    "    while True:\n",
    "        block_length, end = count_mismatches(i)\n",
    "        if printing:\n",
    "            print(f\"Mismatch block: {block_length}\")\n",
    "        assert block_length > 0 or end\n",
    "        ret += encoded_number_size(block_length)\n",
    "        if not block_length:\n",
    "            break\n",
    "        length, encoded_size = encoded_mismatches(i, prev)\n",
    "        i += length\n",
    "        ret += encoded_size\n",
    "        \n",
    "        block_length, end = count_matches(i)\n",
    "        if printing:\n",
    "            print(f\"Match block: {block_length}\")\n",
    "        assert block_length > 0 or end\n",
    "        ret += encoded_number_size(block_length)\n",
    "        if not block_length:\n",
    "            break\n",
    "        length, encoded_size = encoded_matches(i)\n",
    "        i += length\n",
    "        ret += encoded_size\n",
    "        prev = s[i - 1]\n",
    "    \n",
    "    if printing:\n",
    "        if len(s) > 0:\n",
    "            print(f\"{ret}b, {ret / len(s):.2f} bits/char\")\n",
    "        else:\n",
    "            print(f\"{ret}b\")\n",
    "            \n",
    "    \n",
    "    return ret\n",
    "evaluate_predictor(huffman_dict_grouped_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfa378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_log_bits_per_char = 0\n",
    "count = 0\n",
    "for s in manual_test_data:\n",
    "    print(s)\n",
    "    bits = huffman_dict_grouped_hits(s, printing=True)\n",
    "    if bits > 0 and len(s) > 0:\n",
    "        sum_log_bits_per_char += math.log(bits / len(s))\n",
    "        count += 1\n",
    "    print()\n",
    "    \n",
    "print(f\"Geometric mean {math.exp(sum_log_bits_per_char / count):.2f}b/char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a51f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaz_dict = [\"\\002s,\\266\", \"\\003had\\232\\002leW\", \"\\003on \\216\", \"\", \"\\001yS\",\n",
    "\"\\002ma\\255\\002li\\227\", \"\\003or \\260\", \"\", \"\\002ll\\230\\003s t\\277\",\n",
    "\"\\004fromg\\002mel\", \"\", \"\\003its\\332\", \"\\001z\\333\", \"\\003ingF\", \"\\001>\\336\",\n",
    "\"\\001 \\000\\003   (\\002nc\\344\", \"\\002nd=\\003 on\\312\",\n",
    "\"\\002ne\\213\\003hat\\276\\003re q\", \"\", \"\\002ngT\\003herz\\004have\\306\\003s o\\225\",\n",
    "\"\", \"\\003ionk\\003s a\\254\\002ly\\352\", \"\\003hisL\\003 inN\\003 be\\252\", \"\",\n",
    "\"\\003 fo\\325\\003 of \\003 ha\\311\", \"\", \"\\002of\\005\",\n",
    "\"\\003 co\\241\\002no\\267\\003 ma\\370\", \"\", \"\", \"\\003 cl\\356\\003enta\\003 an7\",\n",
    "\"\\002ns\\300\\001\\\"e\", \"\\003n t\\217\\002ntP\\003s, \\205\",\n",
    "\"\\002pe\\320\\003 we\\351\\002om\\223\", \"\\002on\\037\", \"\", \"\\002y G\", \"\\003 wa\\271\",\n",
    "\"\\003 re\\321\\002or*\", \"\", \"\\002=\\\"\\251\\002ot\\337\", \"\\003forD\\002ou[\",\n",
    "\"\\003 toR\", \"\\003 th\\r\", \"\\003 it\\366\",\n",
    "\"\\003but\\261\\002ra\\202\\003 wi\\363\\002</\\361\", \"\\003 wh\\237\", \"\\002  4\",\n",
    "\"\\003nd ?\", \"\\002re!\", \"\", \"\\003ng c\", \"\",\n",
    "\"\\003ly \\307\\003ass\\323\\001a\\004\\002rir\", \"\", \"\", \"\", \"\\002se_\", \"\\003of \\\"\",\n",
    "\"\\003div\\364\\002ros\\003ere\\240\", \"\", \"\\002ta\\310\\001bZ\\002si\\324\", \"\",\n",
    "\"\\003and\\a\\002rs\\335\", \"\\002rt\\362\", \"\\002teE\", \"\\003ati\\316\", \"\\002so\\263\",\n",
    "\"\\002th\\021\", \"\\002tiJ\\001c\\034\\003allp\", \"\\003ate\\345\", \"\\002ss\\246\",\n",
    "\"\\002stM\", \"\", \"\\002><\\346\", \"\\002to\\024\", \"\\003arew\", \"\\001d\\030\",\n",
    "\"\\002tr\\303\", \"\", \"\\001\\n1\\003 a \\222\", \"\\003f tv\\002veo\", \"\\002un\\340\", \"\",\n",
    "\"\\003e o\\242\", \"\\002a \\243\\002wa\\326\\001e\\002\", \"\\002ur\\226\\003e a\\274\",\n",
    "\"\\002us\\244\\003\\n\\r\\n\\247\", \"\\002ut\\304\\003e c\\373\", \"\\002we\\221\", \"\", \"\",\n",
    "\"\\002wh\\302\", \"\\001f,\", \"\", \"\", \"\", \"\\003d t\\206\", \"\", \"\", \"\\003th \\343\",\n",
    "\"\\001g;\", \"\", \"\", \"\\001\\r9\\003e s\\265\", \"\\003e t\\234\", \"\", \"\\003to Y\",\n",
    "\"\\003e\\r\\n\\236\", \"\\002d \\036\\001h\\022\", \"\", \"\\001,Q\", \"\\002 a\\031\", \"\\002 b^\",\n",
    "\"\\002\\r\\n\\025\\002 cI\", \"\\002 d\\245\", \"\\002 e\\253\", \"\\002 fh\\001i\\b\\002e \\v\",\n",
    "\"\", \"\\002 hU\\001-\\314\", \"\\002 i8\", \"\", \"\", \"\\002 l\\315\", \"\\002 m{\",\n",
    "\"\\002f :\\002 n\\354\", \"\\002 o\\035\", \"\\002 p}\\001.n\\003\\r\\n\\r\\250\", \"\",\n",
    "\"\\002 r\\275\", \"\\002 s>\", \"\\002 t\\016\", \"\", \"\\002g \\235\\005which+\\003whi\\367\",\n",
    "\"\\002 w5\", \"\\001/\\305\", \"\\003as \\214\", \"\\003at \\207\", \"\", \"\\003who\\331\", \"\",\n",
    "\"\\001l\\026\\002h \\212\", \"\", \"\\002, $\", \"\", \"\\004withV\", \"\", \"\", \"\", \"\\001m-\", \"\",\n",
    "\"\", \"\\002ac\\357\", \"\\002ad\\350\", \"\\003TheH\", \"\", \"\", \"\\004this\\233\\001n\\t\",\n",
    "\"\", \"\\002. y\", \"\", \"\\002alX\\003e, \\365\", \"\\003tio\\215\\002be\\\\\",\n",
    "\"\\002an\\032\\003ver\\347\", \"\", \"\\004that0\\003tha\\313\\001o\\006\", \"\\003was2\",\n",
    "\"\\002arO\", \"\\002as.\", \"\\002at'\\003the\\001\\004they\\200\\005there\\322\\005theird\",\n",
    "\"\\002ce\\210\", \"\\004were]\", \"\", \"\\002ch\\231\\002l \\264\\001p<\", \"\", \"\",\n",
    "\"\\003one\\256\", \"\", \"\\003he \\023\\002dej\", \"\\003ter\\270\", \"\\002cou\", \"\",\n",
    "\"\\002by\\177\\002di\\201\\002eax\", \"\", \"\\002ec\\327\", \"\\002edB\", \"\\002ee\\353\", \"\",\n",
    "\"\", \"\\001r\\f\\002n )\", \"\", \"\", \"\", \"\\002el\\262\", \"\", \"\\003in i\\002en3\", \"\",\n",
    "\"\\002o `\\001s\\n\", \"\", \"\\002er\\033\", \"\\003is t\\002es6\", \"\", \"\\002ge\\371\",\n",
    "\"\\004.com\\375\", \"\\002fo\\334\\003our\\330\", \"\\003ch \\301\\001t\\003\", \"\\002hab\", \"\",\n",
    "\"\\003men\\374\", \"\", \"\\002he\\020\", \"\", \"\", \"\\001u&\", \"\\002hif\", \"\",\n",
    "\"\\003not\\204\\002ic\\203\", \"\\003ed @\\002id\\355\", \"\", \"\", \"\\002ho\\273\",\n",
    "\"\\002r K\\001vm\", \"\", \"\", \"\", \"\\003t t\\257\\002il\\360\", \"\\002im\\342\",\n",
    "\"\\003en \\317\\002in\\017\", \"\\002io\\220\", \"\\002s \\027\\001wA\", \"\", \"\\003er |\",\n",
    "\"\\003es ~\\002is%\", \"\\002it/\", \"\", \"\\002iv\\272\", \"\",\n",
    "\"\\002t #\\ahttp://C\\001x\\372\", \"\\002la\\211\", \"\\001<\\341\", \"\\003, a\\224\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940719c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(x) + 1 for x in smaz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9226c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "241 * 3 + 141*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
